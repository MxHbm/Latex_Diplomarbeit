\chapter{Computational study}
\label{chap:computational_study}
This chapter will give the most insights into the functionality and efficiency of the usage of the presented binary classifier in
\gls{3L-CVRP} algorithms. This chapter will begin with the selection of two published \gls{3L-CVRP} datasets.
Afterwards the results from the model training are presented, which
is divided in three main parts, firstly the results of the retrieval of the data, secondly the feature selection, and finally presenting
different strategies for comparing different models and selecting the best best fit performance wise.
Afterwards a parameter study is conducted for the \gls{ILS}, starting with the \textit{NoClassifier} variant determining
the best configurations for the base parameters, followed by a variant specific parameter study.
This chapter closes with a computational study of all four variants, comparing the variants with tuned parameters.
These insights are summarized in the concluding chapter.

\section{Comparison of Available Datasets}
\label{sec:dataset_selection}

The \gls{3L-CVRP} is a well-studied problem and several datasets were published in the past, considering
different constraints and characteristics. A selection of these datasets will be compared and evaluated
in this section. The goal is to identify suitable datasets for training a general \gls{CLP} classifier that can predict
the loading feasibility of single tours from different datasets. Therefore, the dataset needs
heterogeneous characterists to represent numerous possible use-cases
as shown in Section~\ref{sec:motivation_feasibility_prediction}. Five published
\cgls{3L-CVRP} datasets are presented with respect to their overall characteristics.
Each dataset gets an unique identfier to simplify the comparison and is shown in parenthesis
after the following individual introduction. The first \cgls{3L-CVRP} dataset was published by \citeauthor{gendreau_tabu_2006} in
\citeyear{gendreau_tabu_2006} and delivered the first \cgls{3L-CVRP} instances containing huge and heavy items (\gendreauDataSet).\footcite[cf.][]{gendreau_tabu_2006}
The second dataset was published by \citeauthor{moura_integrated_2009} in \citeyear{moura_integrated_2009},
and combines the \gls{VRP} from \citeauthor{solomon_algorithms_1987} and the \gls{CLP} instances from
\citeauthor{bischoff_issues_1995} defining the \gls{3L-VRPTW} considering
many items of small size and weight (\mouraDataSet).\footcites[cf.][]{solomon_algorithms_1987,bischoff_issues_1995}[][]{moura_integrated_2009}
The first dataset containing real-life data was published by \citeauthor{ceschia_local_2013} in \citeyear{ceschia_local_2013}
and contains the instances with the most items (\ceschiaDataSet).\footcite[cf.][]{ceschia_local_2013}
Krebs published two different datasets in
\citeyear{krebs_advanced_2021} with a focus on more realistic constraints. The first one contains a set
of realistic constraints and offers a wide range of instance sizes (\krebsADataSet).\footcite[cf.][]{krebs_advanced_2021}
The second one focuses on semi-trailer trucks and special requirements for axle weights (\krebsBDataSet).\footcite[cf.][]{krebs_axle_2021}
The characteristics of the datasets are summarized in the following Table~\ref{tab:dataset_comparison},
where the brackets [\,] indicate a range of possible values. All values considering mass and volume are
\textit{relative} to the respective vehicle weight and volume limit to be comparable. A \textit{item type} is
defined by its geometrical dimensions, the weight, and possible stability characteristics, such as fragility or \gls{LBS}.
When the number of item types is smaller than the number of items equal item types occur multiple times. The types column
depict the number of different item types per instance. Additionaly
most features of the dataset are compared with \textit{aggregated}  values, referring to the aggregrated characteristics
of all items requested by one customer, so the aggregated mass, volume and items shows the average value requested by an
customer of this dataset.

\input{tables/instance_comparison.tex}

The columns routes, route length and fragility show the average over all instances, and
routes define the \gls{LB} for the needed vehicles $K$ and route length the average number of customers per route based
on the relative volume and mass. The averages are displayed to become a better understanding of the
statistics of each dataset, rather than looking at extreme values.
The most important consideration, when selecting a suitable dataset for the training of a classifier,
is how representative single tours from one dataset are for all other datasets. Therefore, the numeric characteristics
should not contain outliers. It is apparent, that the \gendreauDataSetText dataset has the least items per customer
with huge relative volume and weight, which leads with an average route length of 6.22 customers to very few items
considered per route in comparison to the other datasets. This makes it easier to compute the feasibility of the loading
as the combinations of placing patterns is limited. The \mouraDataSetText has the most items per average per customer consisting
of only 5 item types. The \ceschiaDataSetText dataset contains the fewest instances, but with the most maximum items of 8060,
which lead to many routes on average. The two datasets from Krebs, have similar
boundaries and values, but \krebsBDataSetText has routes with twice as many customers as \krebsADataSetText on average due
to the smaller average aggregated mass and volume. Both \krebsADataSetText and \gendreauDataSetText
show a good variety of the features, without including too many items per route in comparison to the other datasets.
However, as the average route length from \krebsADataSetText is twice as big and the number of items per customer also
twice as big, one route contain per average four times more items leading to a higher complexity for solving the \gls{CLP}.
The following Table~\ref{tab:constraint_matrix} provides an overview of the constraints considered
in each dataset showcasing the realistic profile. The constraints are categorized in the five groups introduced
in Section~\ref{sec:clp_definition}.
\clearpage
\input{tables/constraints_table.tex}

This comparison shows that all datasets include similar types of constraints, but the level
of complexity varies. \krebsADataSetText and \ceschiaDataSetText stand out by incorporating
more advanced constraints such as robust stability, reachability, and \gls{LBS}, in comparison to
basic ones like support area, \gls{LIFO} and fragility. To further investigate the differences
between the datasets, Figure~\ref{fig:dataset_comparison} visualizes the aggregated relative mass and
volume of all items requested by individual customers.
Additionally, the size of each scatter point indicates the total number of items requested.
For example, the \mouraDataSetText dataset includes 46
instances with 25 customers each, resulting in $25 \cdot 46 = 1150$ dots in the plot.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.85\textwidth]{pictures/comparison_datasets_3lcvrp.png}
    \caption{Comparison aggregated customer demands of different 3L--CVRP/ 3L--VRPTW datasets.}
    \label{fig:dataset_comparison}
\end{figure}

The dispersion of the data points reflects the diversity of individual instances in terms of volume
and mass dependency. A more balanced profile suggests that some customers tend to order items that
are either mass- or volume-intensive, which supports training the model on more heterogeneous data.
Therefore, the dataset should cover a wide range of cases, varying in mass, volume, and item
quantity per customer. The widest spread is observed in \krebsADataSetText and \gendreauDataSetText serving
both as good dataset candidates for training a classifier. Both datasets are investigated further in
the next section.

\section{Analsis of datasets}
\label{sec:analysis_datasets}

The two datasets, \krebsADataSetText and \gendreauDataSetText, have a good diverse profile for training
a binary classifier and to be further analyzed.
As shown in Section~\ref{sec:literature_overview} several publications solved the \gendreauDataSetText dataset
with various heuristics and even exact approaches, whereas only one heuristic solution approach exists for the instances of Krebs.
Both datasets are further analysed in this section to understand dataset specific properties.

\subsubsection{\krebsADataSetText}

The dataset contains 600 instances with 18 instance types derived from the combinations
of number of customers, item types and items. The following Figure~\ref{fig:krebs_dataset_analysis_detailes} plots
the relative mass and volume of all items requested by individual customers for each of the instances. Every color
represents one instance type and the plots are divided by the number of respective customers in threee groups, presenting
6 combinations each. There are three levels for the different item types per [3,10,100] and two levels for the total
number of items, 200 and 400, per instance.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.85\textwidth]{pictures/krebs_instances_detailed.png}
    \caption[Visualization of different instances of \textcite{krebs_advanced_2021} dataset.]{Visualization of different instances of \krebsADataSetText dataset.
        The instances are named by the number of customers, item types and items.}
    \label{fig:krebs_dataset_analysis_detailes}
\end{figure}

Several insights can be obtained from the analysis of this plot. Firstly, the aggregated relative
volume and mass per customer is significantly lower for the 20 customers group than for the groups with 60 or 100 customers.
Secondly, the distribution differs from each instance type, ranging from quite linear distributions in a narrow
interval (e.g. instance 60-100-400) to quite broad distributions (e.g. instance 100-100-400). These two observations need to be considered,
when selecting instances to generate training data for the classifier to avoid a homogenous training set, and
as a consequence poor classifying results with a low accuracy. The instance set should be drawn from every group equally and
different distributions need to be considered per group, that the average route numeric route structure differs.

\subsubsection{\gendreauDataSetText}

This dataset consists of 27 instances, where the dispersion of the aggregated mass per customer is reaching very high values, up to
0.91, but has modest volume levels, with a maximum of 0.4 approximately, as could be seen in Figure~\ref{fig:dataset_comparison}. The
following Figure~\ref{fig:aggregated_gendreau_plots} show this dispersion per instance revealing an important insight about the dataset.
As the relative volume is quite for all instances, the relative mass differs between the instances. As it was analyzed from \cite{tamke_branch-and-cut_2024}
the complexity to solve the instances is far greater, when the items are more lightweight and the volume is the limiting factor
for packing items in the container. Furthermore, the authors distinguished the instances in a group of heavy items ($\mathcal{H}$) and
a group of lighweight items ($\mathcal{L}$) by dividing the two groups by the average weight utilization $\overline{\omega}$ of the found
groups. If $\overline{\omega} \geq 0.7$ then an instance belongs to $\mathcal{H}$ and for values smaller than 0.7 to $\mathcal{L}$.\footcite[cf.][pp. 23-25]{tamke_branch-and-cut_2024}
The obtained results for the \gendreauDataSetText instances will be also differentiated in those two groups to investigate the effect of
the average weight utilization $\overline{\omega}$ on the solution quality and process.

\begin{figure}[ht]
    \centering
    \begin{tikzpicture}[node distance=0mm and 0mm]
        \node[anchor=south, inner sep=0] (A) at (0,0)
        {\includegraphics[width=0.95\textwidth]{pictures/AggMassCustGendreau.png}};
        \node[anchor=north, below=of A,inner sep=0] (B)
        {\includegraphics[width=0.96\textwidth]{pictures/AggVolCustGendreau.png}};
    \end{tikzpicture}
    \caption{Aggregated relative mass and volume per customer distributed for each instance of the \gendreauDataSetText dataset.}
    \label{fig:aggregated_gendreau_plots}
\end{figure}

As the construction of train datasets with the \krebsADataSetText is much more computationally challenging as for \gendreauDataSetText,
the following sections will focus only on the latter and the Section~\ref{sec:application_krebs} on solving the \krebsADataSet.
Additionally, the Section~\ref{app:sec:krebs_computationally_challenges} in the appendix dives deeper in the challenges for creating a suiting training dataset from the
\krebsADataSetText instances. In the following section the results from training the binary classifier
are presented with diffferent datasets.

\section{Results Training}
\label{sec:ResultsTraining}
In this section the results from the feature dataset selection are discussed. The single best features
with individual scores are shown in the Appendix~\ref{app:feature_selection}. For each model variant (see Section~\ref{sec:modelselection})
the most suiting dataset is selected from both, the Random Retrieval and Save Strategy datasets. These selected
models will then be used in the further steps of the computational study to compare the \gls{ILS} algorithm
with and without the boost of the classifier.

\subsection{Random Retrieval Strategy}
The following random datasets were created with the Algorithm~\ref{fig:flowchart_randomRouteGeneration}.All instances
from \gendreauDataSetText were considered. It must be noted, that the parameter for attempts limit $\beta$ did not have a huge influence on
the number of routes and the structure of the dataset. Therefore only the datasets with $\beta = 30$ have been preselected.
The values in the balance column stand for the share of positive and netative labels in the sample population. The relative volume
and mass refer to the average value for all routes in the respective dataset.

\begin{table}[ht]
    \centering
    \small
    \begin{tabular}{@{}L{0.20\textwidth}P{0.04\textwidth}P{0.04\textwidth}P{0.05\textwidth}P{0.10\textwidth}P{0.12\textwidth}P{0.12\textwidth}P{0.12\textwidth}@{}}
        \toprule
        Name          & $\alpha$           & $\gamma$            & $\delta$ & Routes & Balance   & Rel. Vol & Rel. Mass \\
        \midrule
        RD-2-30-20-6  & \multirow{3}{*}{2} & \multirow{3}{*}{20} & 0.6      & 36779  & 34.4/65.6 & 0.66     & 0.55      \\
        RD-2-30-20-8  &                    &                     & 0.8      & 40724  & 29.0/71.0 & 0.69     & 0.59      \\
        RD-2-30-20-10 &                    &                     & 1        & 47350  & 24.3/75.7 & 0.75     & 0.65      \\
        \midrule
        RD-2-30-30-6  & \multirow{3}{*}{2} & \multirow{3}{*}{30} & 0.6      & 56644  & 33.6/66.4 & 0.66     & 0.55      \\
        RD-2-30-30-8  &                    &                     & 0.8      & 63011  & 28.2/71.8 & 0.69     & 0.59      \\
        RD-2-30-30-10 &                    &                     & 1        & 72408  & 24.2/75.8 & 0.75     & 0.6       \\
        \midrule
        RD-3-30-20-6  & \multirow{3}{*}{3} & \multirow{3}{*}{20} & 0.6      & 48987  & 35.4/64.6 & 0.65     & 0.54      \\
        RD-3-30-20-8  &                    &                     & 0.8      & 61376  & 28.8/71.2 & 0.69     & 0.59      \\
        RD-3-30-20-10 &                    &                     & 1        & 70843  & 24.5/75.5 & 0.74     & 0.64      \\
        \midrule
        RD-3-30-30-6  & \multirow{3}{*}{3} & \multirow{3}{*}{30} & 0.6      & 84751  & 33.8/66.2 & 0.66     & 0.55      \\
        RD-3-30-30-8  &                    &                     & 0.8      & 93942  & 28.6/71.4 & 0.69     & 0.59      \\
        RD-3-30-30-10 &                    &                     & 1        & 108597 & 24.0/76.0 & 0.75     & 0.65      \\
        \midrule
        RD-4-30-20-6  & \multirow{3}{*}{4} & \multirow{3}{*}{20} & 0.6      & 73549  & 34.4/65.6 & 0.66     & 0.55      \\
        RD-4-30-20-8  &                    &                     & 0.8      & 81607  & 28.9/71.1 & 0.69     & 0.59      \\
        RD-4-30-20-10 &                    &                     & 1        & 94666  & 24.2/75.8 & 0.75     & 0.65      \\
        \midrule
        RD-4-30-30-6  & \multirow{3}{*}{4} & \multirow{3}{*}{30} & 0.6      & 113241 & 33.7/66.3 & 0.66     & 0.55      \\
        RD-4-30-30-8  &                    &                     & 0.8      & 125181 & 28.4/71.6 & 0.69     & 0.59      \\
        RD-4-30-30-10 &                    &                     & 1        & 144311 & 24.1/75.9 & 0.75     & 0.65      \\
        \midrule
        RD-5-40-40-6  & \multirow{3}{*}{5} & \multirow{3}{*}{40} & 0.6      & 197525 & 32.0/68.0 & 0.67     & 0.56      \\
        RD-5-40-40-8  &                    &                     & 0.8      & 217085 & 27.3/72.7 & 0.70     & 0.60      \\
        RD-5-40-40-10 &                    &                     & 1        & 249762 & 23.1/76.9 & 0.76     & 0.66      \\
        \bottomrule
    \end{tabular}
    \caption{Created instances for different parameter combinations $(\alpha, \beta, \gamma, \delta)$ for \gendreauDataSetText dataset.}
    \label{tab:created_instances_xyz_gendreau}
\end{table}
The greatest impact on the route characteristics contained in the datasets has parameter $\delta$ as fewer and lighter packed routes
are created. The route characteristics of balance and relative volume and weight are nearly identical for each other tuple combination
of ($\alpha$,$\beta$,$\gamma$). This influence is visualized in the following Figure~\ref{fig:route-dists_randomdata}, where the distribution
of the route length of each $\delta$ variant is shown for an exemplary random dataset.

\begin{figure}[ht]
    \centering
    \begin{subfigure}[t]{.5\textwidth}
        \centering
        \includegraphics[width=\linewidth]{pictures/dataset_structure/no_cust_plot_RandomData_3_30_20_6.png}
        \caption{Threshold $\delta$ = 0.6}
        \label{fig:ds-a}
    \end{subfigure}%
    \begin{subfigure}[t]{.5\textwidth}
        \centering
        \includegraphics[width=\linewidth]{pictures/dataset_structure/no_cust_plot_RandomData_3_30_20_8.png}
        \caption{Threshold $\delta$ = 0.8}
        \label{fig:ds-b}
    \end{subfigure}
    \begin{subfigure}[t]{.5\textwidth}
        \centering
        \includegraphics[width=\linewidth]{pictures/dataset_structure/no_cust_plot_RandomData_3_30_20_10.png}
        \caption{Threshold $\delta$ = 1.0}
        \label{fig:ds-c}
    \end{subfigure}
    \caption{Distribution of route length for the two adapted save strategy datasets. The red vertical line represents the average
        number of customers.}
    \label{fig:route-dists_randomdata}
\end{figure}



\subsubsection{Save Retrieval Strategy}

By running the complete B\&C algorithm from \cite{tamke_branch-and-cut_2024}, two train datasets were generated,
one considering the infeasible routes from the \textit{NoSequence} sets and one only the sequences. The prior dataset is
labeled with WS (= with sets). The length of the routes included in the
train dataset are dominated by routes consisting of 2 customers,
as in the beginning of the algorithm all combinations are checked to find infeasible customers tuple to
fasten the feasibility loading check in the algorithm afterwards. \footcite[cf.][]{tamke_branch-and-cut_2024}
The results of the branch-and-cut algorithm are summarized in Table~\ref{tab:bc_results_gendreau} in the appendix.
This distribution is visualized in the following Figure:
\begin{figure}[ht]
    \centering
    \includegraphics[width = .7\textwidth]{pictures/dataset_structure/no_cust_plot_gendreau_28880_600_WS.png}
    \caption{Customer count per route for the complete save strategy dataset.}
    \label{fig:customer_count_bc}
\end{figure}

To reduce the number of routes with two customers, two altenative datasets are constructed for each base dataset to test if the reduction
of number of routes with 2 customers, lead to better model performance results. Therefore it was tested
to shrink all feasible routes with 2 customers with a cosine similarity of $94\%$ (Shrinked) or alternatively drop
all feasible routes with 2 customers (Trimmed).
The resulting datasets have the following characterics and it is apparent, that the average relative volume
and mass are higher for the two modified datasets, as short routes were dropped. Additionally the balance between
feasible and infeasible is tilting to more to infeasible tours as only feasible routes with two customers were dropped.
Additonally the WS datasets contain more infeasible routes and are therefore more imbalanced. As the modification
of the dataset size only affects the routes with two customers, the column is shown, to
highlight that for example for the Complete-WS dataset round 70.000 routes are dropped.

\begin{table}[ht]
    \centering
    \begin{tabular}{l c c c c c c }
        \toprule
        Name        & Sets                 & Routes & Routes Len = 2 & Balance   & Rel. Vol & Rel. Mass \\
        \midrule
        Complete-WS & \multirow{3}{*}{Yes} & 286858 & 72832          & 37.6/62.4 & 0.62     & 0.46      \\
        Trimmed-WS  &                      & 216260 & 2234           & 17.2/82.8 & 0.74     & 0.54      \\
        Shrinked-WS &                      & 249564 & 35538          & 28.2/71.8 & 0.68     & 0.50      \\        \midrule
        Complete    & \multirow{3}{*}{No}  & 220825 & 72832          & 48.8/51.2 & 0.55     & 0.43      \\
        Trimmed     &                      & 150227 & 2234           & 24.7/75.3 & 0.69     & 0.53      \\
        Shrinked    &                      & 183531 & 35538          & 38.4/61.6 & 0.61     & 0.48      \\

        \bottomrule
    \end{tabular}
    \caption[Save strategy train datsets from \gendreauDataSet.]{Save strategy train datsets from \gendreauDataSet.}
    \label{tab:saved_instances_gendreau}
\end{table}

\subsubsection{Differences between Random and save strategy datasets}

As the creation method differs significantly between the random and the save strategy, differences are laid out in
the following paragraph. The branch-and-cut algorithm does not follow any strict route generation procedure, but to
find the optimal solution for each instance. The complexity to solve these is aligned with the number of routes found.
Therefore only few routes from simple instances, belonging to $\mathcal{H}$, are considered in the train dataset
in comparison to the random datasets.

\begin{figure}[ht]
    \centering
    \begin{subfigure}[t]{.5\textwidth}
        \centering
        \includegraphics[width=\linewidth]{pictures/dataset_structure/distribution_plot_RandomData_5_40_40_10.png}
        \caption{Random data dataset RD-3-30-20-8.}
    \end{subfigure}%
    \begin{subfigure}[t]{.5\textwidth}
        \centering
        \includegraphics[width=\linewidth]{pictures/dataset_structure/distribution_plot_gendreau_28880_600_WS.png}
        \caption{Save strategy dataset Complete-WS.}
    \end{subfigure}
    \caption{Distribution of routes considered per instance for two exemplary datasets of random and save strategy.}
    \label{fig:comparison_noroutes_perInstancce}
\end{figure}

\subsection{Feature Filter Results}
\label{sec:feature_filter_results}

The filter Algorithm~\ref{alg:filter_algorithm} was applied for each of the following minimum importance thresholds
$\epsilon$ and barriers $\mathcal{B}$. Furthermore, the pearson correlation thresholds $\Phi$ = [0.8, 0.85, 0.9] and
all 24 datasets, forming $\mathcal{D}$, presented in the previous subsection were considered.
\begin{table}[ht]
    \centering
    \begin{tabular}{c c c c}
        \toprule
        Drop Set Name & Barrier $\mathcal{B}$ & Threshold $\epsilon$ & Number Features \\
        \midrule
        DropSet-50-2  & 0.5                   & 0.2                  & 10              \\
        DropSet-50-3  & 0.5                   & 0.3                  & 18              \\
        DropSet-50-4  & 0.5                   & 0.4                  & 38              \\
        DropSet-75-2  & 0.75                  & 0.2                  & 7               \\
        DropSet-75-3  & 0.75                  & 0.3                  & 18              \\
        DropSet-75-4  & 0.75                  & 0.4                  & 32              \\\midrule
        DropSet-0-0   & -                     & -                    & 0               \\
        \bottomrule
    \end{tabular}
    \caption{Presentation of the considered drop sets for the feature selection.}
    \label{tab:drop_set_presentation_shortened}
\end{table}

Figure~\ref{fig:feature_filter_parameters} in the appendix shows, how each set is constructed visualizing
different thresholds $\epsilon$ and the influence of the barrier $\mathcal{B}$. It needs to be noted, that
also thresholds below 20\% were considered, but these had no impact on the final subset. The list of the
features which will be dropped are shown in Table~\ref{tab:feature_dropsets}. Fot each dropset each dataset was trained
for each model type and was used to predict the true labels of every other dataset to select the best fitting dropset.
The used hyperparameters for each model can be seen in Table~\ref{tab:hyperparams_feature_selection} in the appendix.
Additionally, one empty dropset was added, to analyse the impact of the dropsets. The following Figure, shows the mean \gls{MCC}
for each variant:

\begin{table}[ht]
    \centering
    \begin{tabular}{c c c c c c c}
        \toprule
        Model                          & Strategy & Name       & \gls{MCC}-Score & \gls{AUROC} & F1-Score & Accuracy \\
        \midrule
        \multirow{2}{*}{\gls{LR}}      & Random   & RD-4-20-30 & 0.6             & 0.87        & 0.9      & 0.8      \\
                                       & Save     & Complete   & 0.6             & 0.87        & 0.9      & 0.8      \\
        \midrule
        \multirow{2}{*}{Decision Tree} & Random   & RD-4-20-30 & 0.6             & 0.87        & 0.9      & 0.8      \\
                                       & Save     & Complete   & 0.6             & 0.87        & 0.9      & 0.8      \\
        \midrule
        \multirow{2}{*}{\gls{FFNN}}    & Random   & RD-4-20-30 & 0.6             & 0.87        & 0.9      & 0.8      \\
                                       & Save     & Complete   & 0.6             & 0.87        & 0.9      & 0.8      \\

        \bottomrule
    \end{tabular}
    \caption{Presentatione of final datasets and feature selection}
    \label{tab:final_dataset_features}
\end{table}

\section{Parameter Study}
\label{sec:parameter_study}

The parameter study is divided in four subsections following the variants of the algorithm
presented in Section~\ref{sec:FeasibilityCheck}. This study follows a hierarchical procedure
tuning the parameters, introduced for each variant, sequentially. The following subset of 13
instances from \gendreauDataSetText is used:


\begin{table}[ht]
    \centering
    \setlength{\tabcolsep}{0.75em}
    \def\arraystretch{1.5}
    \begin{tabular}{lllllll}
        E016-03m & E022-04g & E023-03g & E023-05s & E030-03g & E033-04g & E033-05s \\
        E051-05e & E072-04f & E076-07s & E076-14s & E101-10c & E101-14s &          \\
    \end{tabular}
\end{table}

The division followed the following approach, first all instances were omitted, which found
the optimal solution in a short time (see \cite{tamke_branch-and-cut_2024}\footcite[cf.][p.26]{tamke_branch-and-cut_2024}).
Second, similar instances of size and complexity were reduced to only one.
Every instance is run three times with different seeds and a timelimit of 10 min. It will be investigated
how different parameter combinations influence the \gls{RDP}, the iteration number, the rejection rate and
the average improvement per second after finding the initial solution. The \gls{RDP} is defined between the
total costs of the best solution $C^*$ and another solution with costs $C$.
\begin{align}
    RPD = \frac{C - C^*}{C^*} \cdot 100\%
\end{align}
The rejection rate is the proportion of iterations rejected for infeasible loading in the CP check.
All experiments were run on a AMD EPYC 7513 32-Core machine with 8 cores maximum available for the \gls{CP} solver
and the heuristic is implemented in C++ and all models were pretrained in Python.

\subsection{NoClassifier Variant}
\label{subsec_parameterStuy_noclassifier}
In this variant, all base \gls{ILS} parameters are tested, providing the foundation for subsequent variants,
with the limitation that loading is checked only using the exact \gls{CP} solver. The following levels of
different parameters were selected in a full grid parameter study.

\begin{table}[ht]
    \centering
    \setlength{\tabcolsep}{2em}
    \def\arraystretch{1.1}
    \begin{tabular}{@{}P{4cm}P{8cm}@{}}
        \toprule
        Parametertype      & Levels                                                         \\
        \midrule
        AttemptsLimit $a$  & 3, 5, 8, 12                                                    \\
        RandomMoves R      & 2, 4, 8, 12                                                    \\
        Perturbation       & R-Swaps, R-Insertions, Both-Insertions-First, Both-Swaps-First \\
        Neighborhood order & IntraFirst, InterFirst                                         \\
        \bottomrule
    \end{tabular}
    \caption{Parameter levels for NoClassifier variant.}
    \label{tab:parameters_noclassifier}
\end{table}

The core result is, that the \gls{ILS} without speedups is heavely relying on the number of iterations to find good
solutions. The average \gls{ILS} iterations are very low for some instances as shown in Figure~\ref{fig:average_iterations_noclassifier}.
Therefore, the instances were grouped in two groups dependent if the average iterations are below 25 iterations (Visualized with different bar
colors and the horizontal red line). The heatmap of the parameters RandomMoves and LimitNoImpr (see Figure~\ref{fig:heatmap_parameter_study}) indicate, that the best solutions
were found for the paramters, where either the fewest number of perturbations is executed or the solution is reset frequently to the best found
solutions to have fewer intensification phases from a strong perturbated solution.

\begin{figure}[ht]
    \centering
    \begin{minipage}[t]{0.49\textwidth}
        \centering
        \includegraphics[width=\linewidth]{pictures/iterations_per_instance.png}
        \captionof{figure}{\small Average iterations for each \gendreauDataSetText instance.}
        \label{fig:average_iterations_noclassifier}
    \end{minipage}\hfill
    \begin{minipage}[t]{0.49\textwidth}
        \centering
        \includegraphics[width=\linewidth]{pictures/heatmap_randomMoves_limitNoImpr.png}
        \captionof{figure}{\small Dependency between RPD and RandomMoves and LimitNoImpr}
        \label{fig:heatmap_parameter_study}
    \end{minipage}
\end{figure}

The complete results are shown as boxplots for each parameter and performance metric in the Appendix Section~\ref{app:subsec:parameterstudy_noclassifier}.
This parameterstudy shows, that the order of the local search neighborhoods has the only significant effect and the inter neighborhoods
should be applied first (see Figure~\ref{fig:parameterstudy_NoClassifier_localSearch}). In general it can be observed again, that
the number of iterations matter most, as the results of the two groups introduced differ strongly performance wise. With these results
the parameters for the \gls{ILS} variants using the classifier can not be deduced, as the average number of iterations will be higher due to
speedups incorporated and the rejection rate plays a more significant role. For the comparison between the different variants the following
configuration will be chosen for NoClassifier:

\begin{table}[ht]
    \centering
    \begin{tabular}{@{}cccc@{}}
        \toprule
        AttemptsLimit & RandomMoves        R & Local Search order & Perturbation          \\
        \midrule
        3             & 4                    & InterLSFirst       & Both-Insertions-First \\
        \bottomrule
    \end{tabular}
\end{table}

\subsection{SpeedUp Variant}
\label{subsec_parameterStuy_speedup}

\begin{table}[ht]
    \centering
    \begin{tabular}{c c }
        \toprule
        Parametertype            & Levels     \\
        \midrule
        IterationsWithoutCPCheck & [1,3,5,10] \\
        UseFilterStartSolution   & [1,0]      \\
        \bottomrule
    \end{tabular}
    \caption{Parameter levels for SpeedUp variant.}
    \label{tab:parameters_speedup}
\end{table}

\subsection{Hybrid Variant}
\label{subsec_parameterStuy_hybrid}

\begin{table}[ht]
    \centering
    \begin{tabular}{c c }
        \toprule
        Parametertype             & Levels                      \\
        \midrule
        Hybrid Usage Alternatives & \{[1,1,0],[1,0,1],[0,1,1]\} \\
        UseFilterStartSolution    & [1,0]                       \\
        \bottomrule
    \end{tabular}
    \caption{Parameter levels for hybrid variant.}
    \label{tab:parameters_hybrid}
\end{table}

\section{Comparison of heuristic variants}
\label{sec:comparison_ils_variants}

The solution quality for all four variants (see Fig.~\ref{fig:four_variants}) is compared by running all 27 instances.
The timelimit is in comparison to the parameterstudy now adapted by instance size. The following timelimits are identical
to \cite{zhang_evolutionary_2015}.\footcite[cf.][p.28]{zhang_evolutionary_2015}
\begin{table}[ht]
    \centering
    \begin{tabular}{C{0.24\linewidth}C{0.18\linewidth}C{0.18\linewidth}C{0.18\linewidth}}
        \toprule
        Customer number $N$ & $N \leq 25$ & $N < 25 < 50 $ & $N \geq 50 $ \\
        \midrule
        Timelimit [sec]     & 900         & 1800           & 3600         \\
        \bottomrule
    \end{tabular}
    \caption{Timelimit for the final heuristic comparisons.}
\end{table}

\section{Krebs will be cooked}
\label{sec:application_krebs}