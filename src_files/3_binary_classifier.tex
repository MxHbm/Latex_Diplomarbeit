\chapter{Binary Classifier}
\label{chap:classifier}

The \gls{CVRP} minimizes the total driven distance over all tours. In every tour
a number of customers need to be served with a set of items. When considering multidimensional items
the NP-hard problem itself,increases in complexity, as every tour is representing a \gls{CLP} itself. \footcite[cf.][p. 1f]{tamke_branch-and-cut_2024}
The packing of each route is either feasible or infeasible due to the loading constraints,
creating the possibility to predict the loading status of each tour with a classifier.
These classifiers are supervised learning models, which are trained on a labeled dataset,
where the correct output values are known in advance, and then use this knowledge to
make predictions on new, unseen data. The performance can be evaluated afterwards by comparing
the predicted labels with the actual labels. \footcite[cf.][]{kotsiantis_supervised_2007}
An exemplary train dataset is shown in Table~\ref{tab:classifier_label_data}.

\input{tables/label_classifier}

A classifier can be trained using various \gls{ML} models such as \gls{LR},
\gls{ANN}, support vector machines, etc. However, the most crucial aspect of any
\gls{ML} model is the selection of data, the choice of features and the size of the training set,
as numerous models can be implemented using available libraries and evaluated in terms of performance.
The model attempts to learn correlations between the provided features
and the corresponding labels. If the features are poorly chosen, the model may fail
to capture the underlying patterns in the data. Additionally, if the training set
is too small, the model might not generalize well, ultimately lacking the ability
to accurately predict unseen data. Furthermore, it needs to be noted, that available
\gls{ML} models are not by nature superior to other models, but can significantly outperform
other models on specific application problems. \footcite[cf.][pp. 250, 264]{kotsiantis_supervised_2007}


\section{Evaluation metrics for ML approaches}
\label{sec:classifier_objectives}
To compare different models and \gls{ML} approaches performance indicators are needed. Most measures rely
on the binary output of the confusion matrix, which categorizes predictions by their true and predicted labels.
The goal is to minimize false predicted values.

\begin{table}[ht]
    \centering
    \small
    \begin{tabular}{@{}lcc@{}}
        \toprule
                                 & \textbf{Predicted Positive} & \textbf{Predicted Negative} \\
        \midrule
        \textbf{Actual Positive} & \Gls{TP}                    & \Gls{FN}                    \\
        \textbf{Actual Negative} & \Gls{FP}                    & \Gls{TN}                    \\
        \bottomrule
    \end{tabular}
    \caption{Confusion matrix for binary classification.}
    \label{tab:confusion_matrix}
\end{table}

The dispersion of the single groups, can be used to calculate the accuracy and the F1-score of the classification.
These are standard performance metrics assessing the model between 0 (worst) and 1 (best) behavior:
\begin{align}
    \text{Accuracy}=\frac{TP+TN}{TP+FN+TN+FP}
    \qquad
    \text{F1}=\frac{2\,TP}{2\,TP+FP+FN}
\end{align}
The accuracy is the ratio between the true classified classes against the whole population
of predictions. The F1-score emphasis the \gls{TP} labels and is therefore especially
often used for classification purposes.
However, these evaluation metrics draw too optimistic a picture of the model performance,
when the dataset is imbalanced. Imbalance is occurrent,
when the classes representing each label, feasible and infeasible, have a huge size difference.
Two alternative metrics are, firstly, the \gls{MCC}, which is robust against imbalance
and focuses on \gls{FP} and \gls{FN} values. \gls{MCC} has the advantage to account the majority of the predictions
relative to class size and is defined from -1 to 1. \footcite[cf.][pp. 2--3, 5]{chicco_advantages_2020}
\begin{align}
    \text{MCC}=\frac{TP \cdot TN - FP \cdot FN}{\sqrt{(TP+FP)\cdot(TP+FN)\cdot(TN+FP)\cdot(TN+FN)}}
\end{align}
Secondly, the \gls{AUROC}, which evaluates the overall classification performance by using several thresholds to compute the \gls{TPR}
and \gls{FPR} at different acceptance levels \footcite[cf.][p. 2 f.]{chicco_advantages_2020}. Both the \gls{TPR} and the
\gls{FPR} represent ratios: the \gls{TPR} measures the proportion of correctly classified positive instances relative to all
actual positives, while the \gls{FPR} measures the proportion of incorrectly classified positive instances (false positives)
relative to all actual negatives.
The following Plot~\ref{fig:AUROC_curve} shows several receiver operating curves and their corresponding \gls{AUROC} values.
The baseline is the angle bisector, indicating random guessing with a value of 0.5. The fit gets better
for an increasing area under the curve. The perfect fit is displayed, when the rectangle is covered under the receiver operating curve.

\input{Plots/auroc_plot.tex}

To highlight the differences between an imbalanced and balanced training dataset, two classifications and their respective
evaluation metrics, with exception of the \gls{AUROC}, are compared in the following Table~\ref{tab:indicator_comparison}.

\begin{table}[ht]
    \centering
    \small
    \begin{tabular}{@{}P{0.1\textwidth}P{0.42\textwidth}P{0.42\textwidth}@{}}
        \toprule
        Score  & Balanced Dataset A $(n = 100)$                                                           & Imbalanced Dataset B  $(n = 100)$                                                \\
        \midrule
        Bal.   & Pos = 50; Neg = 50                                                                       & Pos = 91; Neg = 9                                                                \\
        \midrule
        Distr. & TP = 47; FN = 3; TN = 5; FP = 45                                                         & TP = 90; FN = 1; TN = 1; FP = 8                                                  \\
        \midrule
        Acc    & $\frac{47 + 5}{47 + 3  + 5 + 45} = 0.52$                                                 & $\frac{90 + 1}{90 + 1  + 1 +8} = 0.91$                                           \\
        \midrule
        F1     & $\frac{2\cdot 47}{2\cdot47 + 3 + 45} = 0.66$                                             & $\frac{2 \cdot 90}{2 \cdot 90 + 1 +8} = 0.95$                                    \\
        \midrule
        MCC    & $\frac{47\cdot5 -3\cdot45\cdot1}{\sqrt{(47+45)\cdot(47+3)\cdot(5+45)\cdot(3+5)}} = 0.07$ & $\frac{90\cdot1 - 8\cdot1}{\sqrt{(90+8)\cdot(90+1)\cdot(1+8)\cdot(1+1)}} = 0.21$ \\
        \bottomrule
    \end{tabular}
    \caption[Comparison of accuracy, F1-score and MCC with an exemplary balanced and unbalanced
        dataset.]{Comparison of accuracy, F1-score and \gls{MCC} with an exemplary balanced and unbalanced
        dataset.\footnote{Numbers and examples are inspired on \cite[p. 9]{chicco_advantages_2020}}}
    \label{tab:indicator_comparison}
\end{table}

In the first example the positive and negative label classes are equal, and the distribution for the
labels is more or less random, as the sum of the false predicted values is almost equal to
the true predicted values. The accuracy and the \gls{MCC} indicate this random guessing
behavior with values close to the center of the respective indicator range.
The F1-score is slightly positive as the rate of true predicted values of the positive class
with $94\%$ is very high. As the dataset is balanced, classic metrics can still grasp
the poor performance of the classifier. However, when looking at the second example, both
accuracy and F1-score have very high values indicating very good performance, but this is
due to the imbalanced class distribution. Only $11\%$ of the negative class, with 9 samples,
is correctly classified. This poor performance is valued with a low \gls{MCC} score.
It should be noted that, the optimization focus defines the choice of the performance metric.
If the \gls{TPR} is foremost of interest, the F1-score serves well.\footcite[cf.][p. 8f]{chicco_advantages_2020}
The following section provides a more detailed description of how the routes for the training data are retrieved.

\section{Data Retrieval}
\label{sec:DataRetrieval}

Collecting the training data is a crucial step in model creation, and two approaches are introduced in this section.

\subsubsection{Saving labeled routes from exact algorithms (Save-strategy)}
The first approach is to save all routes with exact loading labels from an existing \gls{3L-CVRP}
algorithm, as \cite{zhang_learning-based_2022} did in their paper.\footcite[cf.][]{zhang_learning-based_2022}
To follow this approach, the exact branch-and-cut algorithm from \cite{tamke_branch-and-cut_2024} was
used to save all routes, which are labeled and found in an 8-hour timelimit and with a maximum run time of 10 min for the \gls{CP} solver.
The routes are saved based on their respective loading status and loading flag. The loading status sorts the routes in the
label categories feasible, infeasible, unknown and invalid and is determined by the exact \gls{CP} solver.
If the \gls{CP} solver could not find a deterministic feasible/ infeasible label within the maximum run time, the route
will be classified as unknown or as invalid, if a concrete label must be found. The loading flag defines the subset of
loading constraints used. Three loading flags are considered in the original algorithm, which are shown
in the following:
\begin{itemize}
    \item All Constraints: $\mathcal{G}$
    \item No Support Area: $\mathcal{G}\setminus \{\text{SupportArea}\}$
    \item No Sequence: $\mathcal{G}\setminus \{\text{Sequence}\}$
\end{itemize}
In the case of loading flag \textit{NoSequence}, all routes are independent of a specific customer order, and the customers are stored
without any sequence information. By contrast, routes obtained from the other two flags are stored as concrete sequences.
Since every permutation of a \textit{NoSequence} route corresponds to the same loading status, adding all permutations to the training
dataset would overwhelm the contributions of the other routes. To prevent this imbalance, only two random permutations are
retained per \textit{NoSequence} route. Based on this decision, two versions of the training datasets are constructed:
one including the \textit{NoSequence} routes and one excluding them. These are distinguished by the suffix WS (with sets).\footcites[Retrieved from][]{tamke_repository_2024}[cf.][]{tamke_branch-and-cut_2024}
The following table summarizes all combinations of loading flag and loading status and indicates which routes are included in
the final training datasets.

\begin{table}[ht]
    \centering
    \small
    \begin{tabular}{@{}
            P{0.20\textwidth}
            P{0.20\textwidth} % AllConstraints
            P{0.20\textwidth} % NoSupport
            P{0.20\textwidth} % NoLIFO
            @{}}
        \toprule
        Loading Flag                    & \textbf{All Constraints}     & \textbf{NoSupport}           & \textbf{NoSequence}          \\
        \midrule
        \multirow{3}{*}{Loading Status} & \cellcolor{green!20}Feasible & Feasible                     & Feasible                     \\
                                        & \cellcolor{red!20}Infeasible & \cellcolor{red!20}Infeasible & \cellcolor{red!20}Infeasible \\
                                        & \cellcolor{red!20}Unknown    & \cellcolor{red!20}Unknown    & \cellcolor{red!20}Unknown    \\
                                        & \cellcolor{red!20}Invalid    & \cellcolor{red!20}Invalid    & \cellcolor{red!20}Invalid    \\
        \bottomrule
    \end{tabular}
    \caption[Construction of training data from branch-and-cut routes.]{Construction of training data from branch-and-cut routes. All green cells are labeled as feasible, and all
        red cells as infeasible data.}
    \label{tab:train_data_BC_routes}
\end{table}

Since the trained classifier is applied within an algorithm that accounts for all loading constraints,
feasible tours must satisfy all such constraints. Conversely, a tour that is infeasible under a subset of loading constraints
will also remain infeasible when additional constraints are imposed.\footcite[cf.][p. 7]{tamke_branch-and-cut_2024}
This distinction is reflected in Table~\ref{tab:train_data_BC_routes}.
Because the same route may appear in different groups, for example, being infeasible under both \textit{AllConstraints} and
\textit{NoSupport}, duplicate tours are removed prior to model training. This procedure is referred to as the save-strategy or
B\&C data. In \cite{zhang_learning-based_2022}, a dataset generated by the exact algorithm was used to train a classifier for that same
algorithm. In contrast, the dataset in this thesis is applied to a different algorithm.\footcite[cf.][p. 14]{zhang_learning-based_2022}
Chapter~\ref{chap:computational_study} will examine whether this constitutes a significant disadvantage.

\subsubsection{Creating random routes (Random-strategy)}
An alternative to the first approach is to create random routes based on the \gls{3L-CVRP} instances.
These routes need to be labeled in feasible and infeasible routes regarding their loading.
A \gls{CLP} algorithm with high solution quality is needed, to ensure the correct labels. As the heuristic approaches
presented in Section~\ref{sec:classical_solution_approaches} do not provide an optimal solution,
the best choice is to use an exact approach to determine if all items fit into the container considering the loading constraints.
For this purpose, the exact \gls{CP} model was extracted from \cite{tamke_repository_2024} to create
a standalone \gls{CLP} solver, which can be called for just one route.\footcite[cf.][]{tamke_repository_2024}
The random routes are created with the \gls{RRG} algorithm, which is controlled by four parameters to
influence the resulting number of random routes and their characteristics.\footnote{See Algorithm~\ref{fig:flowchart_randomRouteGeneration}.}
The \gls{RRG} algorithm is presented in detail in Section~\ref{chap:appendix:RRG}.
Hence, the parameters and functionalities are summarized in the following:
\begin{itemize}
    \item 4 Input Parameters
          \begin{itemize}
              \item Multiplier $\alpha$: Integer determining repetitions ($n\cdot\alpha$) for each route length $n$
              \item  Attempts limit $\beta$: Integer determining max failures per repetition
              \item  Success threshold $\gamma$: Objective of succesfully generated routes per repetition
              \item  Lower threshold $\delta$: Float determining the range [$\delta$,1] to draw relative volume and weight capacity thresholds for one route
              \item Instance set $\mathcal{I}$: Set of instances routes should be generated from
          \end{itemize}
    \item Route length starts with two customers and increases until algorihtm breaks
    \item Route duplicates are discarded
    \item Algorithm stops, when no random route could be generated in $n \cdot \alpha$ rounds
\end{itemize}

Generally, the higher the parameters $\alpha$, $\beta$ and $\gamma$ are set and the more instances
are considered in the instance set $\mathcal{I}$, the more routes will be found. When the threshold $\delta$
is smaller than 1, then shorter and fewer routes are selected. Different
random training datasets are presented in the Chapter~\ref{chap:computational_study}.
This strategy will be called \textit{random-strategy}. In the next section three model candidates
are presented more in detail.

\section{Model Selection}
\label{sec:modelselection}
Every supervised learning method can predict the feasibility of a single loading, and the comparison
between different models is a tidious job. Therefore only three different classification
models will be introduced.
Two different types of models will be compared, regression and pure classification models. Regression
models are based on calculations and output a value between 0 and 1. Afterwards, the returning value is classified with
an artifical threshold in two distinct categories, as feasible and infeasible loading. Pure classification
models predict directly the label. \footcite[cf.][p. 5]{nasteski_overview_2017}

\subsubsection{Logistic Regression}

The idea for \gls{LR} is based on linear regression, where either one or multiple independent explanatory
variables $\beta_i$ are multiplied with the feature values $X_i$ to sum up to a continuous scalar $\pi_i$.
The resulting model is the linear equation of the y-axis interception $\beta_0$
and the gradient variables $\beta_i$.
These explanatory variables are fitted to the population of samples by minimizing the mean squared
error. Afterwards the model can be used to predict new data points.\footcite[cf.][p. 6f]{nasteski_overview_2017}
The linear regression formula is depicted in the following, indicating the usage of vectors with bold font.
\begin{align}
    \pi_i=\beta_0+\beta_0 \cdot X_i+\dots+\beta_n \cdot X_n = \beta_0 + \bm{\beta} \times \bm{X}
    \label{eq:base_lr}
\end{align}
Linear regression is a very simple modeling approach and serves as a benchmark for many problems. As the Formula~\ref{eq:base_lr} is only able to predict continuous
values, it is not possible to classify binary values. Therefore the output variable $\pi_i$ is transformed via the sigmoid function, a logistic function,
to model a continuous probability distribution with the formula:
\begin{align}
    \sigma(z)=\frac{1}{1+\exp^{-z}},\, \text{with } z = \beta_0 + \bm{\beta} \cdot \bm{X}
    \label{eq:logistic_func}
\end{align}
The resulting logistic function is bound to the range of 0 and 1, and results can be divided
in two labels using a threshold. An output of 0.5 can be interpreted as random
guessing between both labels, as $z=0$. The curve of the logistic function is shown in Figure~\ref{fig:LR_plot}.
The model is fitted to predict feasible and infeasible labels best.\footcite[cf.][]{kirasich_random_2018}

\input{Plots/logistic_regression_sigmoid.tex}

\subsubsection{Decision Trees}
Decision trees classify samples by sorting them based on their features. Every decision tree
is built up by three elements, one root node and several branches and leaves. The root node
depicts the starting point of the classification and is a branch, classifying the sample
regarding to one feature. At every level of the tree, several branches are added, which can be followed
by either branches or leaves. Each branch splits the instance space into two or more sub-spaces
according to a certain threshold of the input values. A leaf is the exit point declaring
the label of the sample data.\footcite[cf.][p. 5f]{nasteski_overview_2017}
The following small decision tree (see Fig.~\ref{fig:decision_tree}) is an example,
how the loading could be classified with numerical features.

\input{tikz/decision_tree.tex}
The  prominent advantage is, that the logic behind the classification can be understood
completely. In this example all routes, with more than 30 items and $70\%$ volume utilization are classified infeasible,
as well as those tours where the relative weight utilization is above $80\%$, but the volume utilization is lower. The complex
$NP$-hard problem is to find the optimal binary decision tree and finding the optimal subset and order of
features.
When many levels of branches are added, and the decision tree grows in depth, the chance of
overfitting the decision tree to the training data is more likely. Therefore, several control mechanisms
were developed to avoid this. The first one is to control the maximum depth (\textit{max\_depth})
of the decision tree. Secondly, after the decision tree is created, to prune it by removing leaves from
the tree and comparing it with an identical descision tree, if the overall performance has decreased.
The third method is, stopping the fitting algorithm before the data is perfectly fitted to. \footcite[cf.][p. 252]{kotsiantis_supervised_2007}
One further advantage of decision trees are, that the data does not need to to be scaled as every feature is
distinguished on feature own thresholds, in comparion of the other presented models. In this thesis the \gls{XGB} model is used,
which is an advanced \gls{ML} model based on decision trees, where several decision trees are created. In several iterations
the overall error of the classification is reduced by increasing the importance of previously misclassified predicted labels. \footcite[cf.][]{noauthor_introduction_2025}

\subsubsection{Feed Forward Neural Networks}

Neural networks are a collection of single perceptrons/neurons, which are internally connected by
signals. In the case of \gls{FFNN}, signals are only allowed to go straight from input to output. One neuron
in the network is either defined as input, output or as hidden unit. One neuron calculates its
output similar to the \gls{LR} with $X_i$ feature values and $w_i$ weights, the output is defined
as $\sum_{i \in n} X_i \cdot w_i$ and can be truncated by a threshold forcing all values above 1
to be 1 and all negative values to be zero. Every unit signals its calculated output by an individual
activation function to all units of the next layer of the network. All signals are
then interpreted at the output layer. For binary classification tasks the presented sigmoid function from \gls{LR} is used.\footcite[cf.][p. 255]{kotsiantis_supervised_2007}
An exemplary \gls{FFNN} is shown in Figure~\ref{fig:ffnn}.

\input{tikz/feed_forward_model.tex}

The training and selection of the single layers is a complex task
and modern libraries as PyTorch are supporting with prebuilt models and analysis functions to choose
a good setup. Generally, a \gls{FFNN} can be considered as an advanced \gls{LR} model,
which also uses nonlinear functions to process signals.

\parbreak
\cite{kotsiantis_supervised_2007} states, that no supervised \gls{ML} method is superior to another,
but each has its own advantages and disadavantages, which need to be considered.
Therefore, all three model types are included and compared in this work
to understand the complexity of the underlying classification problem. As \gls{FFNN} normally
tend to need a large sample size and perform good with continuous data, decision trees can be
trained on much smaller samples and perform good with mixed datatypes consisting of discrete
and continuous values.\footcite[cf.][pp. 262--264]{kotsiantis_supervised_2007}
\gls{LR} is a standard \gls{ML} model and the most straightforward to implement and to train, but can dominate
other approaches due to their simplicity and interoperability. \footcite[cf.][p. 8]{kirasich_random_2018}
However, the training procedure is identical across models, except that the training data for the \gls{LR} and \gls{FFNN}
models must be normalized with $\mathcal{N}(0,1)$. The mean and standard deviation used
for this scaling must be stored, since any new, unseen dataset must first be transformed
using these scaling parameters, before it can be processed. Every model type is trained by applying a 5-fold cross validation.
Resampling methods, like k-fold cross validation, help reducing the bias in modeling, by using at least more than one train testsplit of the data.
The data is split in k similar sized data folds, every fold will be tested with the model trained on the other folds.
This ensures, that the choice of the train test split is not random and results are more rigorous. \footcite[cf.][pp. 69--72]{kuhn_applied_2016}
In the following section the features, used for training the single models, are presented.


\section{Features}
\label{sec:Features}

Features are the most important puzzle piece for training a \gls{ML} model, as they portray the reality in numerical values, trying to
reduce the complexity without the loss of meaningfulness. In the presented case, the loading feasibility probability of single tours.
The complete feature list spans 48 features, which can be divided in three groups to understand the underlying logic.

\begin{itemize}
    \item 2 General features: General information about each tour
    \item 10 Loading constraint features: Concepted to depict the loading constraint set $\mathcal{G}$
    \item 36 Geometrical ratio features: Contain several geometrical ratios of items and container size
\end{itemize}

Some additional definitions to Section~\ref{sec:mathematical_formulation}
are necessary to define the features formally. Following the mathematical formulation, every route consists of the customers
from the subset $S \subseteq C$. Every customer requests items of the set of item types $M$ in different quantities.
Each item type $m_j\in M$ has its own dimensions (height $z_j$, length $x_j$, width $y_j$), weight $q_j$ and fragility flag $f_j$.
The fragility flag contains binary values of either 0 (nonfragile) or 1 (fragile).
Each customer $i$ demands $d_{ij}$ units of item type $j$.
The total weight, volume and quantity requested by a customer $i$ or the whole subset $S$ is calculated then by:

\[q(i) = \sum_{j \in M} q_j \cdot d_{ij}\;\text{, resp. } q(S) = \sum_{i\in S} q(i)\]
\[v(i) = \sum_{j \in M} v_j \cdot d_{ij}\;\text{, resp. } v(S) = \sum_{i\in S} v(i)\]
\[d(i) = \sum_{j \in M} d_{ij}\;\text{, resp. } D(S) = \sum_{i\in S} d(i)\]

Furthermore, it is necessary to define the homogeneous fleet of vehicles
further. Every vehicle $k$ has the same dimensions of height $z_k$, length $x_k$ and width $y_k$ resulting in the maximum volume limit per
vehicle $Q = x_k \cdot y_k\cdot z_k$. As all vehicles are identical, the indices are obsolent, but are kept to distinguish clearer
from the item properties.

\subsubsection{General Features}
These two feature contain the number of customers $|S|$ in the respective route without the depot and the total number of items requested by
this subset of customers $D(S)$.

\subsubsection{Loading Constraint Features}

The features are presented in the Table~\ref{tab:loading_constraints_features} with the mathematical formula as well as the set of
loading constraints this feature is belonging to. The distinction, which loading constraints are described by which feature, is not
unambiguously, as several features can be adressed. The range of values is given in the last column with $h$ representing any number in $\mathbb{N}^{+}$.
All of the features presented have only values relative to the instance specific dimensions and weight limit of the vehicle, as these values
differ between different datasets, but also within the \gendreauDataSetText dataset. \footcite[cf.][p. 346]{gendreau_tabu_2006}
The first two features capture the weight and volume capacity utilization. Features three to five display the sum of one item dimension ($z_j$,$x_j$,$y_j$)
from all items relative to the corresponding container dimension ($z_k$,$x_k$,$y_k$). The fragile ratio describes the share of fragile
items. Capuring the \gls{LIFO} constraint is the most challenging task as multidimensionality and order is complex to numerically describe
with one value. Therefore, certain customer specific values, as relative volume/ weight or the share of fragile items, are multiplied
with the order in which the customers are driven to (Features 7-10). \textit{Fragile Sequence} depicts the difficulty to load fragile items
in the beginning as further items need to be likely placed upon the first items. The three sequence-dependent features, volume balance,
weight distribution, and volume distribution, combine each customer’s requested volume and weight with that stop’s position on
the route to capture the incremental load added at each step.
\begin{table}[!ht]
    \centering
    \small
    \renewcommand{\arraystretch}{2.0}
    \begin{tabular}{@{}P{0.08\textwidth}P{0.16\textwidth}P{0.26\textwidth}P{0.28\textwidth}@{}P{0.1\textwidth}@{}}
        \toprule
        No & Name                                                                                             & Constraint(s)          & Calculation                                                                                           & Range   \\
        \midrule
        1  & Relative Volume                                                                                  & Loading Capacity       & $\displaystyle\frac{v(S)}{V}$                                                                         & [0,1]   \\
        2  & Relative Weight                                                                                  & Loading Capacity       & $\displaystyle\frac{q(S)}{Q}$                                                                         & [0,1]   \\
        3  & Total Relative Length Items\footcite[Feature is adapted from][p. 21]{sarah_de_wolf_machine_2022} & Stability, Orientation & $\displaystyle\frac{1}{x_k} \cdot \sum_{i \in S}\sum_{j \in M} d_{ij} \cdot x_j$                      & [0,$h$] \\
        4  & Total Relative Width Items \footnotemark[\value{footnote}]                                       & Stability, Orientation & $\displaystyle\frac{1}{y_k} \cdot \sum_{i \in S}\sum_{j \in M} d_{ij} \cdot y_j$                      & [0,$h$] \\
        5  & Total Relative Height Items  \footnotemark[\value{footnote}]                                     & Stability, Capacity    & $\displaystyle\frac{1}{z_k} \cdot \sum_{i \in S}\sum_{j \in M} d_{ij} \cdot z_j$                      & [0,$h$] \\
        6  & Ratio Fragile Items                                                                              & Fragility              & $\displaystyle\frac{1}{D(S)} \cdot \sum_{i \in S}\sum_{j \in M} d_{ij} \cdot f_j$                     & [0,1]   \\
        7  & Fragile Sequence                                                                                 & Fragility, \gls{LIFO}  & $\displaystyle\sum_{i \in S}p_i \cdot \frac{1}{d(i)}\sum_{j\in M} d_{ij}\cdot f_j $                   & [0,$h$] \\
        8  & Volume Balance                                                                                   & Capacity, \gls{LIFO}   & $\frac{\displaystyle\sum\nolimits_{i \in S}p_i \cdot v(i)}{\displaystyle\sum\nolimits_{i \in S}v(i)}$ & [0,$h$] \\
        9  & Volume Distribution                                                                              & Capacity, \gls{LIFO}   & $\displaystyle\frac{1}{V}\cdot\sum_{i \in S}p_i \cdot v(i)$                                           & [0,$h$] \\
        10 & Weight Distribution                                                                              & Capacity, \gls{LIFO}   & $\displaystyle\frac{1}{Q}\sum_{i \in S}p_i \cdot q(i)$                                                & [0,$h$] \\
        \bottomrule
    \end{tabular}
    \caption{Loading constraint features.}
    \label{tab:loading_constraints_features}
\end{table}


\subsubsection{Geometrical Ratio Features}
The third and last feature group
was partly used by \cite{zhang_learning-based_2022} and is expanded for additional geometrical ratios applicable to three-dimensional loading.\footcite[cf.][p. 14]{zhang_learning-based_2022}
All geometrical ratios express the relation between two geometrical properties of either one item or one item to the vehicle. These ratios
are calculated for each item demanded from customers $S$ and afterwards, the minimum, maximum, mean and standard deviation is calculated
from the $D(S)$ ratios, which are used as features. The following Figure~\ref{fig:geometrical_ratio_features} visualizes all nine geometrical ratio features.


\begin{figure}[ht]
    \centering

    % ---- Row 1: ratios between item dims & against container (L/H, W/H, W/L, L/CL, H/CH) ----
    \PanelItemDimDim{Width Height \\ \footnotesize{[0,$h$]}}{3}{2}
    \PanelItemDimDim{Length Height\\ \footnotesize{[0,$h$]}}{1}{2}
    \PanelItemDimDim{Width Length\footnote{Features adapted from \cite[p.14]{zhang_learning-based_2022}.}\\ \footnotesize{[0,$h$]}}{2}{3}
    \vspace{25pt}
    \PanelHeightOverArea{Height Area\\ \footnotesize{[0,1]}}
    \vspace{25pt}
    \PanelItemVsCont{Length Container Length\footnotemark[\value{footnote}]\\ \footnotesize{[0,1]}}{1}
    \PanelItemVsCont{Height Container Height\footnotemark[\value{footnote}]\\ \footnotesize{[0,1]}}{2}
    \PanelItemVsCont{Width Container Width\\ \footnotesize{[0,1]}}{3}
    \PanelAreaOverContArea{ItemArea ContainerArea\footnotemark[\value{footnote}]\\ \footnotesize{[0,1]}}
    \PanelVolumeOverContVolume{ItemVolume ContainerVolume\\ \footnotesize{[0,1]}}
    \caption[Nine geometric ratio feature icons.]{Nine geometric ratio feature icons. In each panel, the \textcolor{numC}{numerator is green}
        and the \textcolor{denC}{denominator is red}.
        \label{fig:geometrical_ratio_features}}
\end{figure}

These nine geometrical features grasp the geometrical description of all items and is further outlined in the following example:
In a route the following three items are requested, described by their geometrical dimensions \{height, length, width\}:
\[
    \mathcal{I} = \{(12,5,10),\,(10,3,6),\,(10,10,10)\}
\]
For every item the width length ratio is calculated and the resulting ratio vector is used to calculate the minimum and maximum value,
as well as the mean and the standard deviation:
\[
    \text{Width--Length Ratios: }
    \frac{y_j}{x_j} = \left\{\frac{10}{5},\,\frac{6}{3},\,\frac{10}{10}\right\} = \{2,\,2,\,1\}
\]
\[
    \begin{array}{c|c|c|c}
        \text{Min} & \text{Max} & \text{Mean} & \text{Std. Dev.} \\
        \hline
        1          & 2          & 1.67        & 0.471            \\
    \end{array}
\]

For every geometrical ratio the four features are calculated with this approach.
The complete list of 48 features is shown in the appendix in Table~\ref{tab:complete_features_list}. The next section
presents methods, how to select a subset of features.

\section{Feature Selection}
\label{sec:feature_engineering}

The selection of a subset of features helps to simplify the model by increasing the interpretability and the computation time
for the features. Furthermore, \gls{ML} model types can be negatively affected by non-informative features, which are not correlated
to the prediction outcome. This applies to all models, where all predicting values are combined in a statistical formula
and are considered in the output value, as \gls{LR} and \gls{FFNN}. However, all tree- and rule-based models, as decision trees, are resistant
to the negative influence of non-informative features as not every feature is used for branching. \footcite[cf.][pp. 487--489]{kuhn_applied_2016}
The following Figure~\ref{fig:feature-performance} visualizes how a performance metric, as accuracy, develops depending if the model is
resistant to a high number of non-informative features.
\input{Plots/feature_seletction.tex}

The feature selection approaches for supervised learning models can be divided in three groups, filter, wrapper and embedded methods.
Filter methods are ranking techniques, which are used to select a feature subset before the model is trained by analyzing
the dependency between the features and the output variable. By defining a threshold or an explicit number of features, the
resulting subset is selected. Mutual correlation between features exists and is not considered by filter methods
leading to redundany. Furthermore, the datatype of $X$(input data) and $Y$(output data) determines which filter method is applicable. The risk
of overfitting is reduced as the selection of features is conducted without respecting the model performance.
Wrapper methods are evaluating the model performance with different subsets of features. As the possible number of combinations is growing
exponentially, choosing the right subset is a $NP$-hard problem. Several heuristic methods are used to tackle this task, among them
are sequential selection algorithms, adding respective removing one feature after each other,
\glspl{GA} or branch-and-bound algorithms. The main difference to filter methods is the focus on the model performance.
Embedded approaches aim to reduce the computation time needed for the wrapper methods by integrating the
feature selection in the training process. \footcites[cf.][pp. 17--21]{chandrashekar_survey_2014} As this thesis will analyse several
different datasets and models, the combinations of dataset model tuples for which a perfect feature subset
has to be selected are numerous. Therefore, the same subset of features will be used for each model and dataset to provide comparability.
An adapted filter method is used therefore with the goal to find a subset of features strongly dependent
to the output label, but with low mutual correlation between the features. The following Figure~\ref{fig:overview_Feature_selection}
gives an overview over the methods and narrows down the possible applicable methods.

\input{tikz/overview_feature_selection.tex}

A general approach was published by \cite{hall_feature_1997},
where a joint numerator $G_s$ was constructed of the correlation between the input data $X$ of the selected group $s$ and the output data $Y$.
The optimization goal is to find the optimal group $s^*$ maximizing $G_{s^*}$ having high correlation to $Y$ and low
correlation among $X$ in the group $s^*$. The correlation was measured by calculating the conditional entropy, which is independent
from the data type (numerical, ordinal, categorical) of input and output data. Unfortunately, no heuristic for finding the optimal
feature subset was given. \footcite[cf.][p. 856]{hall_feature_1997} Another approach was presented from \cite{haindl_feature_2006},
where features are discarded from the feature set based on the accumulated mutual correlation between features. However, the correlation between
the features and the output variables is not respected and this approach could lead to non-informative features. \footcite[cf.][]{haindl_feature_2006}
To avoid determining the maximum number of features beforehand, which is required for solely applied filter methods,
a own feature selection Algorithm~\ref{alg:filter_algorithm} is presented, which follows the principle of the two previous approaches.
The procedure filters all high mutually correlated features, with a low dependency score, as well as all features with an
importance score below a threshold $\epsilon$.

\input{algorithms/feature_filter.tex}

The dependency/importance between the output and features $\mathcal{R}_{d,\kappa}(\mathcal{F})$ is computed either with the
\gls{MI} or the \gls{F-Score}. Both scores explain the dependency of numerical feature data to a categorical output value (see Fig.~\ref{fig:overview_Feature_selection}),
but have different scales. As \gls{MI} is defined in [0,1] and \gls{F-Score} in $\mathbb{R}_0^+$, the scores are standardized by $\mathcal{N}(0,1)$, to
apply the same thresholds. \gls{MI} is defined as the impact of a feature $f$ to reduce the uncertainty of the output variable $Y$.
The advantage over classical linear correlation techniques is, that also nonlinear relations can be mapped.\footcite[cf.][p. 539f]{battiti_using_1994}
The \gls{F-Score} is based on the \gls{ANOVA} and computes the linear dependency between the features and the output value.\footcite[cf.][p. 2]{gu_generalized_2012}
Both methods are used to distinguish, if linear or non-linear dependencies exists between $X$ and $Y$. The mutual correlation between the variables
is computed with the \textit{pearson} correlation coefficient applicable to pure numerical data and the formula is shown below.\footcite[cf.][p. 17]{chandrashekar_survey_2014}
\begin{gather*}
    \text{Pearson Correlation Coefficient: } C(f,f') = \frac{Covariance(X_f,X_{f'})}{\sqrt{Variance(X_f)\cdot Variance(X_{f'})}}
\end{gather*}

Several different filtered subsets can be obtained for varying the filter method $\kappa$, the correlation thresholds $\Phi$, the barrier $\mathcal{B}$ and
different quantile thresholds $\epsilon$. By applying the adapted correlation filter\footnote{See Algorithm~\ref{alg:filter_algorithm}.}, the number of final
features does not need to be defined beforehand, as for usual filter methods, but returns the features, which should be excluded, for
various parameters. This allows more flexibility, but the performance of all models needs to be analyzed afterwards.
The following Figure~\ref{fig:normal_distribution} highlights that all features, which have a scaled importance score below $z_{\epsilon}$ are added to the drop candidates, trimming
effectively non-important and possibily non-informative features. Different subsets of features are presented in Chapter~\ref{chap:computational_study} and the following Chapter
presents the used metaheuristic.

\input{Plots/normal_dist.tex}