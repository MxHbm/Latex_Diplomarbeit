\chapter{Binary Classifier}
\label{chap:classifier}

A practical application of the \gls{CLP} is the integration in the \gls{VRP}, where
a number of customers need to be served with a set of items by a fleet of vehicles that have
to start from a depot and return. The goal is to minimize the total distance driven
by the vehicles. When considering multidimensional items, the NP-hard problem itself,
increases in complexity, as every tour is representing a \gls{CLP} itself. \footcite[cf.][pp. 1--2]{tamke_branch-and-cut_2024}

The emphasis is placed primarily on \gls{ML}-based classifiers.
These are supervised \gls{ML} algorithms predicting the
value of a categorical or binary output column, called label, based on the
values of other columns, called features. Classifiers learn from a labeled dataset,
where the correct output values are known in advance, and then use this knowledge to
make predictions on new, unseen data. The accuracy can be evaluated afterwards by comparing
the predicted labels with the actual labels. \footcite[cf.][]{kotsiantis_supervised_2007}
An exemplary train dataset is shown in Table~\ref{tab:classifier_label_data}.

\input{tables/label_classifier}

A classifier can be implemented using various \gls{ML} models such as \gls{LR},
\gls{ANN}, support vector machine, or others. However, the most crucial aspect of any
\gls{ML} model is the selection of data, particularly the choice of features and
the size of the training set, since many models can be easily preselected from available
libraries and be compared performance wise. The model attempts to learn correlations between the provided features
and the corresponding labels. If the features are poorly chosen, the model may fail
to capture the underlying patterns in the data. Additionally, if the training set
is too small, the model might not generalize well, ultimately lacking the ability
to accurately predict unseen data. Furthermore, it needs to be noted, that available
\gls{ML} models are not by nature superior to other models, but can significantly outperform
other models on specific application problem \footcite[cf.][pp. 250, 264]{kotsiantis_supervised_2007}.


\subsubsection{Objectices for ML approaches}
To compare different models and \gls{ML} approaches performance indicators are needed. The most common
measures rely on the binary output of the confusion matrix, which sorts the output along
their true and predicted labels. The predicted labels of the \gls{ML} model is divided in two groups via a certain threshold, usually 0.5, with all predicted values above it, considered in the predictive postive column and below in the predicted negative column.

\begin{table}[ht]
    \centering
    \begin{tabular}{@{}lcc@{}}
        \toprule
                                 & \textbf{Predicted Positive} & \textbf{Predicted Negative} \\
        \midrule
        \textbf{Actual Positive} & True Positive (TP)          & False Negative (FN)         \\
        \textbf{Actual Negative} & False Positive (FP)         & True Negative (TN)          \\
        \bottomrule
    \end{tabular}
    \caption{Confusion matrix (binary classification).}
    \label{tab:confusion_matrix}
\end{table}
The output can be used to calculate the accuracy and F1 score of the classification, which
helps to interpret the outcomes and compare different model types.

\[
    \text{Accuracy}=\frac{TP+TN}{TP+TN+FP+FN}
    \qquad
    \text{F1}=\frac{2\,TP}{2\,TP+FP+FN}
\]
However these classical indicators draw a too optimistic picture of the model performance, when the dataset is imbalanced, as one class is dominating the other. Imbalance is occurrent, when the classes representing each label, feasible and infeasible, have a huge size difference.
Two alternatives are firstly the \gls{MCC}, which is robust against imbalance and needs a certain threshold to divide the classifier output in the positive and negative predicted.
\[
    \text{MCC}=\frac{TP*TN - FP*FN}{\sqrt{(TP+FP)*(TP*FN)*(TN+FP)*(TN+FN)}}
\]
Secondly the \gls{AUROC}, which is the integral below the
receiver operating curve, where no threshold is needed. The latter indicates how
well the classification does in general terms.\footcite[cf.][p.2f.]{chicco_advantages_2020}


\section{Data Retrieval}
\label{sec:DataRetrieval}
Collecting the training data is one crucial step, for the model creation and two approaches exist.

\subsubsection{Saving labeled routes from exact algorithms}
The first one is to save all routes with exact loading label from an exisiting \gls{3L-CVRP}
algorithm, as \cite{zhang_learning-based_2022} did in his paper. However, in his approach it
had the advantage, that the binary model was supposed to be used in the same exact algorithm
and the solution structure of routes in the train data is similar to the real data afterwards. \footcite[cf.][]{zhang_learning-based_2022}
To follow this approach the exact branch-and-cut algorithm from \cite{tamke_branch-and-cut_2024} was
used to save all routes, which are labeled and found in a certain timelimit from an instance. During
the algorithm labeled routes are sorted based on the loading status and loading flag, when all
constraints are considered (loading flag = All Constraints), every labeled route has an underlying
sequence, which is crucial for the label, as the \gls{LIFO} constraint is dominating. However,
when the problem is relaxed constraint wise,
\footcites[Retrieved from][]{tamke_repository_2024}[cf.][]{tamke_branch-and-cut_2024} Every route
is sorted into a set composed of the loading flag and loading status. The flag represents which
subset of constraints was used for this tour and three flags were considered:
\begin{itemize}
    \item All Constraints $\mathcal{G}$
    \item No Support $\mathcal{G}\setminus \{\text{SupportArea}\}$
    \item No LIFO $\mathcal{G}\setminus \{\text{LIFO}\}$
\end{itemize}

The status defines the label for the loading retrived from the \gls{CP} Solver and can be either
Feasible, Infeasible, or Unknown. If the solver could not find a concrete Feasible/ Infeasible
label after a certain time limit the route will be classified as invalid. The following table
represents, which subsets of the loading flags and status are used for the train dataset.

\begin{table}[ht]
    \centering
    \begin{tabular}{@{}
            P{0.20\textwidth} % AllConstraints
            P{0.20\textwidth} % NoSupport
            P{0.20\textwidth} % NoLIFO
            @{}}
        \toprule
        \textbf{All Constraints}     & \textbf{NoSupport}           & \textbf{NoLIFO}              \\
        \midrule
        \cellcolor{green!20}Feasible & Feasible                     & Feasible                     \\
        \cellcolor{red!20}Infeasible & \cellcolor{red!20}Infeasible & \cellcolor{red!20}Infeasible \\
        \cellcolor{red!20}Unknown    & \cellcolor{red!20}Unknown    & \cellcolor{red!20}Unknown    \\
        \bottomrule
    \end{tabular}
    \label{tab:train_data_BC_routes}
    \caption{Constrution of training data from branch-and-cut routes. All green cells are interpreted as feasible, and all
        red cells as infeasible data}
\end{table}

As the trained classifier is used in an algorithm considering all loading constraints, feasible tours
must fulfill all loading constraints, but infeasible tours from a subset of loading constraints
will to a very high probability also be infeasible, when more loading constraints are considered.\footcite[cf.][p.7]{tamke_branch-and-cut_2024}
As the same tour can occur in different infeasible / unknown sets it is important to delete
duplicated tours with the same label before training the model.

\subsubsection{Creating random routes}
An alternative to the first approach, and is the only option when lacking a suiting complete algorithm, is to
create random routes from the \gls{3L-CVRP} instances used. As these random routes lack
a true label, if the corresponding loading of this route is feasible or infeasible, each random
route needs to be labeled individually. One of the approaches presented in Section~\ref{sec:classical_solution_approaches} can be chosen
for the \gls{CLP}, but the solution quality of heuristics must be high, or the best choice is to
use an exact approach to determine, if all items fit into the container with the loading constraints.
For this purpose, the exact \gls{CP} model was extracted from \cite{tamke_repository_2024} to create
a \gls{CP} solver, which can be called for just one route.\footcite[Stolen with permission from][]{tamke_repository_2024}
As there are no control mechanisms in comparison to the exact algorithm, the Algorithm~\ref{alg:rand_routes_generation} was created to test different parameter
combinations. As the pseudocode has a lot of details, the most important characteristics and
functionalities are summarized in the following:

\begin{itemize}
    \item 4 Input Parameters
          \begin{itemize}
              \item Multiplier $\alpha$: Integer determining repetitions ($n\times\alpha$) for each route length $n$
              \item  Attempts limit $\beta$: Integer determining max failures per repetition
              \item  Success threshold $\gamma$: Integer determining success threshold per repetition
              \item Instance set $\mathcal{I}$: Set of instances routes should be generated from
          \end{itemize}
    \item Route length is starting with two customers increasing until no routes can be found
    \item Every new random route is checked, if the route is already contained in a route set
    \item Algorithm stops when no random route could be drawn in $n \times \alpha$ rounds
\end{itemize}

Generally the higher the parameters $\alpha$, $\beta$ and $\gamma$ are set and the more instances
are considered in the instance set $\mathcal{I}$, the more routes will be found. Different
random training datasets are presented in the Chapter~\ref{chap:computational_study}.

\section{Model Selection}
\label{sec:modelselection}
Every \gls{ML} model can predict the feasibility of a single loading, and the comparison between different models and
could fill this whole thesis. Therefore only three different models will be explained in detail and compared in the computational
study about their performance.

\subsubsection{Logistic Regression}

\gls{LR} is based on either one or multiple independent input features, which output the response varibable. The resulting
varibable is the linear equation of one intercept with the y-axis $\beta_0$ and many gradient variables $\beta_i$.
\begin{align}
    \pi_i=\beta_0+\beta_0*X_i+\dots+\beta_n*X_n = \beta_0 + \bm{\beta} * \bm{X}
    \label{eq:base_lr}
\end{align}
is a very simple modeling approach and serves as a benchmark for many problems. When multiple variables are concerned for
fitting the curve the approach from single variable can be adapted. As the Formula~\ref{eq:base_lr} is able to depict continuous
values as output, it is struggling for the needed binary classification. Therefore the response variable $\pi_i$ is transformated
to output a continuous probability distribution. The transformation is done by a logistic function with the formula:
\begin{align}
    \sigma(Z)=\frac{1}{1+\exp^{-z}},\, \text{with } z = \beta_0 + \bm{\beta} * \bm{X}
    \label{eq:logistic_func}
\end{align}
The resulting logistic function lookn like this and it is apparent, that when $z$ is 0, or all input variables are 0, the model
outouts 0.5, which can be interpreted as random guess between both labels feasible and feasible.
\begin{figure}
    \centering
    \begin{tikzpicture}
        \begin{axis}[
                axis lines=middle,
                xlabel={$z$},
                ylabel={$\sigma(z)$},
                samples=200,
                domain=-6:6,
                ymin=-0.2, ymax=1.2,
                xtick={-6,-4,-2,0,2,4,6},
                ytick={0,0.5,1},
                width=10cm,
                height=6cm,
            ]
            % Logistic function
            \addplot[blue, thick] {1/(1+exp(-x))};

            % Dotted lines for y=0 and y=1
            \addplot[dashed, gray] coordinates {(-6,0) (6,0)};
            \addplot[dashed, gray] coordinates {(-6,1) (6,1)};
        \end{axis}
    \end{tikzpicture}
    \caption{Transformation for logistic regression to classify binary output.}
\end{figure}

By applying this transformation the \gls{LR} model can be pretrained to obtain the values for $\beta_0$ and $\bm{\beta}$ and
the function $\sigma(z)$ can be applied to classify new unseen routes.\footcite[cf.][]{kirasich_random_2018}


\subsubsection{Decision Trees}
Decision trees classify samples by sorting them based on their features. Every decision tree
is built up by three elements, one root node and several branches and leaves. The root node
depicts the starting point of the classification and is a branch, classifying the sample
regarding to one feature. At every level of the tree several branches are added, which can be followed
by either branches or leafes. A leaf is the exit point declaring
the label of the sample data. The following small decision tree (see Fig.~\ref{fig:decision_tree}) is an example, how the loading could be classified with numerical features.

\input{tikz/decision_tree.tex}
The  prominent advantage is, that the logic behind the classification can be understood completely all routes, with more than 30 items and $70\%$ volume utilization are infeasible classified, as well as those tours where the relative weight utilization is above $80\%$, but the volume utilization is lower. The complex
$NP$-hard problem is to find the optimal binary decision tree and finding the optimal subset and order of
features.
When many levels of branches are added, and the decision tree grows in depth, the chance of
overfitting the decision tree to the training data is more likely. Therefore several control mechanisms
were developed to avoid this. The first one is to control the maximum depth (\textit{max\_depth})
of the decision tree, second after the decision tree is created to prune it by removing leaves from
the tree and testing with an identical descision tree, if this has lowered the overall performance
and the third method is stopping the fitting algorithm before the data is perfectly fitted to. \footcite[cf.][p.252]{kotsiantis_supervised_2007}
One advantage of decision trees is that the data does not need to to be scaled as every feature is
tested on feature own thresholds.

\subsubsection{Feed Forward Neural Networks}

Neural networks are a collection of single perceptrons/ neuron, which all are internally connected and
in the case of feed forward networks signals are only allowed to go straight from input to output. One neuron
in the network is either defined as input, output or as hidden units. One neuron calculates its
output similar to the \gls{LR} with $X_i$ feature values and $w_i$ weights, the output is defined
as $\sum_{i \in \mathcal{n}} X_i * w_i$ and can be truncated by a threshold forcing all about above 1
to be 1 and all negative values to be zero. Every unit signals its output calculated by an individual
activation function to all the next units in the next layer of the network. All the signals are
then interpreted at the output layer and a result is gained based on all signals. For binary classification tasks the presented sigmoid function from \gls{LR} is used. An exemplary neural network is shown in Figure~\ref{fig:ffnn}.\footcite[cf.][p.255]{kotsiantis_supervised_2007}

\input{tikz/feed_forward_model.tex}

The training of the single layers and deciding which layers should be used is a complex task
and modern libraries as PyTorch are supporting with prebuil and analysis functions to choose a good setup. Generally a \gls{FFNN} could be considered as an advanced \gls{LR} model, which allows also nonlinear functions to be used for the sending of signals.

\parbreak
\cite{kotsiantis_supervised_2007} states, that no supervised \gls{ML} method is superior to another, but each has its own advantages and disadavantages, which need to be considered. Therefore all three different types are included in this work to compare performances, and to understand the complexity of the underlying classification problem. As \gls{FFNN} normally tend to need a large sample size and perform
good with continuous data, decision trees can be trained on much smaller samples and perform good with mixed datatypes consisting of discrete/binary and continuous values.\footcite[cf.][pp. 262ff.]{kotsiantis_supervised_2007} \gls{LR} is a standard method and the easiest to implement and to train, and can dominate other approaches due to their simplicity and interoperability. \footcite[cf.][p.8]{kirasich_random_2018}
In the following section the features used for training the single models are presented. In Chapter~\ref{chap:computational_study} the performance of the single models is compared.

\begin{comment}
\section{Model Training}
\label{sec:ModelTraining}

Training is based on which model selected, XGBoost or /gls{FFNN} from pytorch library or others. Afterwards the Mini batch descent
algorithm to train the model!
\end{comment}

\section{Features}
\label{sec:Features}

Features are the most important puzzle piece for training a \gls{ML} model, as they portray the reality in numerical values, trying to
reduce the complexity without the loss of meaningfulness, in the presented case the laoding feasibility probability of single tours.
The complete feature list spans 48 features, which can be divided in three groups to understand the underlying logic.

\begin{enumerate}
    \item 2 General Features: General information about each tour
    \item 10 Loading Constraint Features: Featues concepted to depict loading constraint set $\mathcal{G}$
    \item 36 Geometrical Ratio Features: Contain several geometrical ratio of items and container size to retrieve
          statistical values of minimum, maximimum, mean and standard deviation
\end{enumerate}

All features are described more in detail in the following. Some additional definitions to Section~\ref{sec:mathematical_formulation}
are necessary to define the features formally. Following the mathematical definition, every route consists of the customers
from the subset $S \subseteq C$. From all customers $C$ the set of item types $M$ is requested, where each item type $m_j$ has its own
dimension {$z_j$, $x_j$, $y_j$}, weight $q_j$ and fragility flag $f_j$. The loading flag contains binary values of either 0 (Nonfragile)
or 1 (Fragile). Each customer $i$ demands $d_{ij}$ units of item type $j$.
The total weight and volume requested by a customer $i$ is calculated then by:

\[q(i) = \sum_{j \in M} q_j * d_{ij}\;\text{, resp. } v(i) = \sum_{j \in M} v_j * d_{ij}\]
\[q(S) = \sum_{i\in S} q(i)\;\text{, resp. } v(S) = \sum_{i\in S} v(i)\]

The quanitity of all items requested by one customer is $d_i = \sum_{j \in M} d_{ij}$ and in total for all customers,
the demand is $D(S) = \sum_{i\in S} d_i$. Furthermore it is necessary to define the homogeneous fleet of vehicles
further. Every vehicle $k$ has the same dimensions {$x_k$, $y_k$, $z_k$} resulting in the maximum volume limit per
vehicle $Q = x_k *y_k*z_k$. As every used vehicle is identical, the indcies are obsolent, but are kept to distinguish more clearly
from item properties.

\subsubsection{General Features}
These two feature contain the number of customers in the respective without the depot $|S|$, and the total number of items requested by
this subset of customers $D(S)$.

\subsubsection{Loading Constraint Features}

To simplifiy the representation the constraints are given in a table with the formula calculating the feature as well as the set of
loading constraints this feature is belonging to. The distinction, which loading constraints are described by one feature, is quite
challenging as some measures are for almost all features important, as the Total Relative Length. The range of possible values is
given with $h$ representing any number in $\mathbb{N}^{+}$.

\begin{table}[ht!]
    \centering
    \renewcommand{\arraystretch}{2.0}
    \begin{tabular}{@{}P{0.08\textwidth}P{0.16\textwidth}P{0.26\textwidth}P{0.28\textwidth}@{}P{0.1\textwidth}@{}}
        \toprule
        No & Name                                                                                            & Constraint(s)          & Calculation                                                               & Range   \\
        \midrule
        1  & Relative Volume                                                                                 & Loading Capacity       & $\displaystyle\frac{v(S)}{V}$                                             & [0,1]   \\
        2  & Relative Weight                                                                                 & Loading Capacity       & $\displaystyle\frac{q(S)}{Q}$                                             & [0,1]   \\
        3  & Total Relative Length Items\footcite[Feature is adapted from][p.21]{sarah_de_wolf_machine_2022} & Stability, Orientation & $\displaystyle\frac{1}{x_k} * \sum_{i \in S}\sum_{j \in M} d_{ij} * x_j$  & [0,$h$] \\
        4  & Total Relative Width Items \footnotemark[\value{footnote}]                                      & Stability, Orientation & $\displaystyle\frac{1}{y_k} * \sum_{i \in S}\sum_{j \in M} d_{ij} * y_j$  & [0,$h$] \\
        5  & Total Relative Height Items  \footnotemark[\value{footnote}]                                    & Stability, Capacity    & $\displaystyle\frac{1}{z_k} * \sum_{i \in S}\sum_{j \in M} d_{ij} * z_j$  & [0,$h$] \\
        6  & Ratio Fragile Items                                                                             & Fragility              & $\displaystyle\frac{1}{D(S)} * \sum_{i \in S}\sum_{j \in M} d_{ij} * f_j$ & [0,1]   \\
        7  & Fragile Sequence                                                                                & Fragility, \gls{LIFO}  & $\displaystyle\sum_{i \in S}p_i * \frac{1}{d_i}\sum_{j\in M} d_{ij}*f_j $ & [0,$h$] \\
        8  & Volume Distribution                                                                             & Capacity, \gls{LIFO}   & $\frac{\sum_{i \in S}p_i * \frac{v(i)}{V}}{\sum_{i \in S}\frac{v(i)}{V}}$ & [0,$h$] \\
        9  & Volume Balance                                                                                  & Capacity, \gls{LIFO}   & $\displaystyle\sum_{i \in S}p_i * \frac{q(i)}{Q}$                         & [0,$h$] \\
        10 & Weight Distribution                                                                             & Capacity, \gls{LIFO}   & $\displaystyle\sum_{i \in S}p_i * \frac{q(i)}{Q}$                         & [0,$h$] \\
        \bottomrule
    \end{tabular}
    \caption{Loading constraints related features}
    \label{tab:loading_constraints_features}
\end{table}

All the features presented have only values to the instance specific dimensions and weight limit of the vehicle, as these values
differ usually between different datasets, but also within the \gendreauDataSetText dataset. \footcite[cf.][p. 346]{gendreau_tabu_2006}
Capuring the \gls{LIFO} constraint is the most challenging task as multidimensionality or order is complexer to describe with a single
numeric value. Therefore certain customer specific values, as relative volume/ weight or the share of fragile items, is multiplied
with the order in which the customers are driven to. \textit{Fragile Sequence} should depict the difficulty to load a lot of fragile items
in the beginning as several other customer demands need to be likely placed upon the first items. The third and last feature group
was partly used by \cite{zhang_learning-based_2022} and is expanded for useful geometrical ratios when considering three-dimensional
instead of two-dimensional loading.\footcite[cf.][p. 14]{zhang_learning-based_2022}

\subsubsection{Geometrical Ratio Features}

All geometrical ratios express the relation between two dimensions of either one item or one item and the vehicle. In total 9 ratios
are considered and for each ratio the min(), max(), meand() and std() values of the vector of values is oevrhanded as model feature.


\begin{figure}
    \centering

    % ---- Row 1: ratios between item dims & against container (L/H, W/H, W/L, L/CL, H/CH) ----
    \PanelItemDimDim{Width Height \\ \footnotesize{[0,$h$]}}{1}{2}
    \PanelItemDimDim{Length Height\\ \footnotesize{[0,$h$]}}{3}{2}
    \PanelItemDimDim{Width Length\footcite[Featues adapted from][p.14]{zhang_learning-based_2022}\\ \footnotesize{[0,$h$]}}{1}{3}
    \vspace{25pt}
    \PanelHeightOverArea{Height Area\\ \footnotesize{[0,1]}}
    \vspace{25pt}
    \PanelItemVsCont{Length Container Length\footnotemark[\value{footnote}]\\ \footnotesize{[0,1]}}{3}
    \PanelItemVsCont{Height Container Height\footnotemark[\value{footnote}]\\ \footnotesize{[0,1]}}{2}
    \PanelItemVsCont{Width Container Width\\ \footnotesize{[0,1]}}{1}
    \PanelAreaOverContArea{ItemArea ContainerArea\footnotemark[\value{footnote}]\\ \footnotesize{[0,1]}}
    \PanelVolumeOverContVolume{ItemVolume ContainerVolume\\ \footnotesize{[0,1]}}
    \caption[Nine geometric ratio feature icons.]{Nine geometric ratio feature icons. In each panel, the \textcolor{numC}{numerator is green}
        and the \textcolor{denC}{denominator is red}.}
\end{figure}

All these nine geometrical features grasp the statistical description of the lot of all items and is shown in the following example:
When we consider a route, where these three items, described by their three dimensions, are requested:
\[
    \mathcal{I} = \{(10,5,12),\,(6,3,10),\,(10,10,10)\}
\]

\[
    \text{Width--Length Ratios: }
    \frac{x_j}{y_j} = \{2,\,2,\,1\}
\]

\[
    \begin{array}{c|c|c|c}
        \text{Min} & \text{Max} & \text{Mean}               & \text{Std. Dev.} \\
        \hline
        1          & 2          & \tfrac{5}{3} \approx 1.67 & 0.471            \\
    \end{array}
\]

For every geometrical ratio the four features are calculated with this simple approach and are added to the model.

\parbreak

As the number of 48 features is huge for describing a rather simple problem in comparison to picture detection, the next steps
involve the analysis, which features have the greatest impact and which features can be dropped due to high correlation to other
features or to irrelevance to the prediction accuracy of the binary classifier.

\section{Feature Selection}
\label{sec:feature_engineering}

The selection of a subset of features presented in Section~\ref{sec:Features} helps to simplify the result and preserving the
same performance. For this procedure several possibilities are available:
\begin{enumerate}
    \item Forward selection
    \item Backward Selection
    \item
\end{enumerate}
