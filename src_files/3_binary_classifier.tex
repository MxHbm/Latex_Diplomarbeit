\chapter{Binary Classifier}
\label{chap:classifier}

A practical application of the \gls{CLP} is the integration in the \gls{VRP}, where
a number of customers need to be served with a set of items by a fleet of vehicles that have
to start from a depot and return. The goal is to minimize the total distance driven
by the vehicles. When considering multidimensional items, the NP-hard problem itself,
increases in complexity, as every tour is representing a \gls{CLP} itself. \footcite[cf.][pp. 1--2]{tamke_branch-and-cut_2024}

The emphasis is placed primarily on \gls{ML}-based classifiers.
These are supervised \gls{ML} algorithms predicting the
value of a categorical or binary output column, called label, based on the
values of other columns, called features. Classifiers learn from a labeled dataset,
where the correct output values are known in advance, and then use this knowledge to
make predictions on new, unseen data. The accuracy can be evaluated afterwards by comparing
the predicted labels with the actual labels. \footcite[cf.][]{kotsiantis_supervised_2007}
An exemplary train dataset is shown in Table~\ref{tab:classifier_label_data}.

\input{tables/label_classifier}

A classifier can be implemented using various \gls{ML} models such as \gls{LR},
\gls{ANN}, support vector machine, or others. However, the most crucial aspect of any
\gls{ML} model is the selection of data, particularly the choice of features and
the size of the training set, since many models can be easily preselected from available
libraries and be compared performance wise. The model attempts to learn correlations between the provided features
and the corresponding labels. If the features are poorly chosen, the model may fail
to capture the underlying patterns in the data. Additionally, if the training set
is too small, the model might not generalize well, ultimately lacking the ability
to accurately predict unseen data. Furthermore, it needs to be noted, that available
\gls{ML} models are not by nature superior to other models, but can significantly outperform
other models on specific application problem \footcite[cf.][pp. 250, 264]{kotsiantis_supervised_2007}.


\subsubsection{Objectices for ML approaches}
To compare different models and \gls{ML} approaches identical objectives are needed. The most common
measures rely on the binary output of the confusion matrix, which sorts the output along
their true and predicted labels.

\begin{table}[ht]
    \centering
    \begin{tabular}{@{}lcc@{}}
        \toprule
                                 & \textbf{Predicted Positive} & \textbf{Predicted Negative} \\
        \midrule
        \textbf{Actual Positive} & True Positive (TP)          & False Negative (FN)         \\
        \textbf{Actual Negative} & False Positive (FP)         & True Negative (TN)          \\
        \bottomrule
    \end{tabular}
    \caption{Confusion matrix (binary classification).}
    \label{tab:confusion_matrix}
\end{table}
The output can be used to calculate the accuracy and F1 score of the classification, which
helps to interpret the outcomes and compare different model types.

\[
    \text{Accuracy}=\frac{TP+TN}{TP+TN+FP+FN}
    \qquad
    \text{F1}=\frac{2\,TP}{2\,TP+FP+FN}
\]


\section{Data Retrieval}
\label{sec:DataRetrieval}
Collecting the training data is one crucial step, for the model creation and two approaches exist.

\subsubsection{Saving labeled routes from exact algorithms}
The first one is to save all routes with exact loading label from an exisiting \gls{3L-CVRP}
algorithm, as \cite{zhang_learning-based_2022} did in his paper. However, in his approach it
had the advantage, that the binary model was supposed to be used in the same exact algorithm
and the solution structure of routes in the train data is similar to the real data afterwards. \footcite[cf.][]{zhang_learning-based_2022}
To follow this approach the exact branch-and-cut algorithm from \cite{tamke_branch-and-cut_2024} was
used to save all routes, which are labeled and found in a certain timelimit from an instance. During
the algorithm labeled routes are sorted based on the loading status and loading flag, when all
constraints are considered (loading flag = All Constraints), every labeled route has an underlying
sequence, which is crucial for the label, as the \gls{LIFO} constraint is dominating. However,
when the problem is relaxed constraint wise,
\footcites[Retrieved from][]{tamke_repository_2024}[cf.][]{tamke_branch-and-cut_2024} Every route
is sorted into a set composed of the loading flag and loading status. The flag represents which
subset of constraints was used for this tour and three flags were considered:
\begin{itemize}
    \item All Constraints $\mathcal{G}$
    \item No Support $\mathcal{G}\setminus \{\text{SupportArea}\}$
    \item No LIFO $\mathcal{G}\setminus \{\text{LIFO}\}$
\end{itemize}

The status defines the label for the loading retrived from the \gls{CP} Solver and can be either
Feasible, Infeasible, or Unknown. If the solver could not find a concrete Feasible/ Infeasible
label after a certain time limit the route will be classified as invalid. The following table
represents, which subsets of the loading flags and status are used for the train dataset.

\begin{table}[ht]
    \centering
    \begin{tabular}{@{}
            P{0.20\textwidth} % AllConstraints
            P{0.20\textwidth} % NoSupport
            P{0.20\textwidth} % NoLIFO
            @{}}
        \toprule
        \textbf{All Constraints}     & \textbf{NoSupport}           & \textbf{NoLIFO}              \\
        \midrule
        \cellcolor{green!20}Feasible & Feasible                     & Feasible                     \\
        \cellcolor{red!20}Infeasible & \cellcolor{red!20}Infeasible & \cellcolor{red!20}Infeasible \\
        \cellcolor{red!20}Unknown    & \cellcolor{red!20}Unknown    & \cellcolor{red!20}Unknown    \\
        \bottomrule
    \end{tabular}
    \label{tab:train_data_BC_routes}
    \caption{Constrution of training data from branch-and-cut routes. All green cells are interpreted as feasible, and all
        red cells as infeasible data}
\end{table}

As the trained classifier is used in an algorithm considering all loading constraints, feasible tours
must fulfill all loading constraints, but infeasible tours from a subset of loading constraints
will to a very high probability also be infeasible, when more loading constraints are considered.\footcite[cf.][p.7]{tamke_branch-and-cut_2024}
As the same tour can occur in different infeasible / unknown sets it is important to delete
duplicated tours with the same label before training the model.

\subsubsection{Creating random routes}
An alternative to the first approach, and is the only option when lacking a suiting complete algorithm, is to
create random routes from the \gls{3L-CVRP} instances used. As these random routes lack
a true label, if the corresponding loading of this route is feasible or infeasible, each random
route needs to be labeled individually. One of the approaches presented in Section~\ref{sec:classical_solution_approaches} can be chosen
for the \gls{CLP}, but the solution quality of heuristics must be high, or the best choice is to
use an exact approach to determine, if all items fit into the container with the loading constraints.
For this purpose, the exact \gls{CP} model was extracted from \cite{tamke_repository_2024} to create
a \gls{CP} solver, which can be called for just one route.\footcite[Stolen with permission from][]{tamke_repository_2024}
As there are no control mechanisms in comparison to the exact algorithm, the Algorithm~\ref{alg:rand_routes_generation} was created to test different parameter
combinations. As the pseudocode has a lot of details, the most important characteristics and
functionalities are summarized in the following:

\begin{itemize}
    \item 4 Input Parameters
          \begin{itemize}
              \item Multiplier $\alpha$: Integer determining repetitions ($n\times\alpha$) for each route length $n$
              \item  Attempts limit $\beta$: Integer determining max failures per repetition
              \item  Success threshold $\gamma$: Integer determining success threshold per repetition
              \item Instance set $\mathcal{I}$: Set of instances routes should be generated from
          \end{itemize}
    \item Route length is starting with two customers increasing until no routes can be found
    \item Every new random route is checked, if the route is already contained in a route set
    \item Algorithm stops when no random route could be drawn in $n \times \alpha$ rounds
\end{itemize}

Generally the higher the parameters $\alpha$, $\beta$ and $\gamma$ are set and the more instances
are considered in the instance set $\mathcal{I}$, the more routes will be found. Different
random training datasets are presented in the Chapter~\ref{chap:computational_study}.

\section{Model Training}
\label{sec:ModelTraining}

Training is based on which model selected, XGBoost or /gls{FFNN} from pytorch library or others. Afterwards the Mini batch descent
algorithm to train the model!

\section{Features}
\label{sec:Features}

Features are the most important puzzle piece for training a \gls{ML} model, as they portray the reality in numerical values, trying to
reduce the complexity without the loss of meaningfulness, in the presented case the laoding feasibility probability of single tours.
The complete feature list spans 48 features, which can be divided in three groups to understand the underlying logic.

\begin{enumerate}
    \item 2 General Features: General information about each tour
    \item 10 Loading Constraint Features: Featues concepted to depict loading constraint set $\mathcal{G}$
    \item 36 Geometrical Ratio Features: Contain several geometrical ratio of items and container size to retrieve
          statistical values of minimum, maximimum, mean and standard deviation
\end{enumerate}

All features are described more in detail in the following. Some additional definitions to Section~\ref{sec:mathematical_formulation}
are necessary to define the features formally. Following the mathematical definition, every route consists of the customers
from the subset $S \subseteq C$. From all customers $C$ the set of item types $M$ is requested, where each item type $m_j$ has its own
dimension {$z_j$, $x_j$, $y_j$}, weight $q_j$ and fragility flag $f_j$. The loading flag contains binary values of either 0 (Nonfragile)
or 1 (Fragile). Each customer $i$ demands $d_{ij}$ units of item type $j$.
The total weight and volume requested by a customer $i$ is calculated then by:

\[q(i) = \sum_{j \in M} q_j * d_{ij}\;\text{, resp. } v(i) = \sum_{j \in M} v_j * d_{ij}\]
\[q(S) = \sum_{i\in S} q(i)\;\text{, resp. } v(S) = \sum_{i\in S} v(i)\]

The quanitity of all items requested by one customer is $d_i = \sum_{j \in M} d_{ij}$ and in total for all customers,
the demand is $D(S) = \sum_{i\in S} d_i$. Furthermore it is necessary to define the homogeneous fleet of vehicles
further. Every vehicle $k$ has the same dimensions {$x_k$, $y_k$, $z_k$} resulting in the maximum volume limit per
vehicle $Q = x_k *y_k*z_k$. As every used vehicle is identical, the indcies are obsolent, but are kept to distinguish more clearly
from item properties.

\subsubsection{General Features}
These two feature contain the number of customers in the respective without the depot $|S|$, and the total number of items requested by
this subset of customers $D(S)$.

\subsubsection{Loading Constraint Features}

To simplifiy the representation the constraints are given in a table with the formula calculating the feature as well as the set of
loading constraints this feature is belonging to. The distinction, which loading constraints are described by one feature, is quite
challenging as some measures are for almost all features important, as the Total Relative Length. The range of possible values is
given with $h$ representing any number in $\mathbb{N}^{+}$.

\begin{table}[ht!]
    \centering
    \renewcommand{\arraystretch}{2.0}
    \begin{tabular}{@{}P{0.08\textwidth}P{0.16\textwidth}P{0.26\textwidth}P{0.28\textwidth}@{}P{0.1\textwidth}@{}}
        \toprule
        No & Name                                                                                            & Constraint(s)          & Calculation                                                               & Range   \\
        \midrule
        1  & Relative Volume                                                                                 & Loading Capacity       & $\displaystyle\frac{v(S)}{V}$                                             & [0,1]   \\
        2  & Relative Weight                                                                                 & Loading Capacity       & $\displaystyle\frac{q(S)}{Q}$                                             & [0,1]   \\
        3  & Total Relative Length Items\footcite[Feature is adapted from][p.21]{sarah_de_wolf_machine_2022} & Stability, Orientation & $\displaystyle\frac{1}{x_k} * \sum_{i \in S}\sum_{j \in M} d_{ij} * x_j$  & [0,$h$] \\
        4  & Total Relative Width Items \footnotemark[\value{footnote}]                                      & Stability, Orientation & $\displaystyle\frac{1}{y_k} * \sum_{i \in S}\sum_{j \in M} d_{ij} * y_j$  & [0,$h$] \\
        5  & Total Relative Height Items  \footnotemark[\value{footnote}]                                    & Stability, Capacity    & $\displaystyle\frac{1}{z_k} * \sum_{i \in S}\sum_{j \in M} d_{ij} * z_j$  & [0,$h$] \\
        6  & Ratio Fragile Items                                                                             & Fragility              & $\displaystyle\frac{1}{D(S)} * \sum_{i \in S}\sum_{j \in M} d_{ij} * f_j$ & [0,1]   \\
        7  & Fragile Sequence                                                                                & Fragility, \gls{LIFO}  & $\displaystyle\sum_{i \in S}p_i * \frac{1}{d_i}\sum_{j\in M} d_{ij}*f_j $ & [0,$h$] \\
        8  & Volume Distribution                                                                             & Capacity, \gls{LIFO}   & $\frac{\sum_{i \in S}p_i * \frac{v(i)}{V}}{\sum_{i \in S}\frac{v(i)}{V}}$ & [0,$h$] \\
        9  & Volume Balance                                                                                  & Capacity, \gls{LIFO}   & $\displaystyle\sum_{i \in S}p_i * \frac{q(i)}{Q}$                         & [0,$h$] \\
        10 & Weight Distribution                                                                             & Capacity, \gls{LIFO}   & $\displaystyle\sum_{i \in S}p_i * \frac{q(i)}{Q}$                         & [0,$h$] \\
        \bottomrule
    \end{tabular}
    \caption{Loading constraints related features}
    \label{tab:loading_constraints_features}
\end{table}

All the features presented have only values to the instance specific dimensions and weight limit of the vehicle, as these values
differ usually between different datasets, but also within the \gendreauDataSetText dataset. \footcite[cf.][p. 346]{gendreau_tabu_2006}
Capuring the \gls{LIFO} constraint is the most challenging task as multidimensionality or order is complexer to describe with a single
numeric value. Therefore certain customer specific values, as relative volume/ weight or the share of fragile items, is multiplied
with the order in which the customers are driven to. \textit{Fragile Sequence} should depict the difficulty to load a lot of fragile items
in the beginning as several other customer demands need to be likely placed upon the first items. The third and last feature group
was partly used by \cite{zhang_learning-based_2022} and is expanded for useful geometrical ratios when considering three-dimensional
instead of two-dimensional loading.\footcite[cf.][p. 14]{zhang_learning-based_2022}

\subsubsection{Geometrical Ratio Features}

All geometrical ratios express the relation between two dimensions of either one item or one item and the vehicle. In total 9 ratios
are considered and for each ratio the min(), max(), meand() and std() values of the vector of values is oevrhanded as model feature.


\begin{figure}
    \centering

    % ---- Row 1: ratios between item dims & against container (L/H, W/H, W/L, L/CL, H/CH) ----
    \PanelItemDimDim{Width Height \\ \footnotesize{[0,$h$]}}{1}{2}
    \PanelItemDimDim{Length Height\\ \footnotesize{[0,$h$]}}{3}{2}
    \PanelItemDimDim{Width Length\footcite[Featues adapted from][p.14]{zhang_learning-based_2022}\\ \footnotesize{[0,$h$]}}{1}{3}
    \vspace{25pt}
    \PanelHeightOverArea{Height Area\\ \footnotesize{[0,1]}}
    \vspace{25pt}
    \PanelItemVsCont{Length Container Length\footnotemark[\value{footnote}]\\ \footnotesize{[0,1]}}{3}
    \PanelItemVsCont{Height Container Height\footnotemark[\value{footnote}]\\ \footnotesize{[0,1]}}{2}
    \PanelItemVsCont{Width Container Width\\ \footnotesize{[0,1]}}{1}
    \PanelAreaOverContArea{ItemArea ContainerArea\footnotemark[\value{footnote}]\\ \footnotesize{[0,1]}}
    \PanelVolumeOverContVolume{ItemVolume ContainerVolume\\ \footnotesize{[0,1]}}
    \caption[Nine geometric ratio feature icons.]{Nine geometric ratio feature icons. In each panel, the \textcolor{numC}{numerator is green}
        and the \textcolor{denC}{denominator is red}.}
\end{figure}

All these nine geometrical features grasp the statistical description of the lot of all items and is shown in the following example:
When we consider a route, where these three items, described by their three dimensions, are requested:
\[
    \mathcal{I} = \{(10,5,12),\,(6,3,10),\,(10,10,10)\}
\]

\[
    \text{Width--Length Ratios: }
    \frac{x_j}{y_j} = \{2,\,2,\,1\}
\]

\[
    \begin{array}{c|c|c|c}
        \text{Min} & \text{Max} & \text{Mean}               & \text{Std. Dev.} \\
        \hline
        1          & 2          & \tfrac{5}{3} \approx 1.67 & 0.471            \\
    \end{array}
\]

For every geometrical ratio the four features are calculated with this simple approach and are added to the model.

\parbreak

As the number of 48 features is huge for describing a rather simple problem in comparison to picture detection, the next steps
involve the analysis, which features have the greatest impact and which features can be dropped due to high correlation to other
features or to irrelevance to the prediction accuracy of the binary classifier.

\section{Feature Engineering}
\label{sec:feature_engineering}
